#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨è‚¡ç¥¨åˆ†æè„šæœ¬

åŠŸèƒ½ï¼š
- åœ¨éäº¤æ˜“æ—¶é—´ï¼ˆ17:00-æ¬¡æ—¥6:00ï¼‰è‡ªåŠ¨éšæœºåˆ†æè‚¡ç¥¨
- é¿å…é‡å¤åˆ†æåŒä¸€åªè‚¡ç¥¨
- æ™ºèƒ½æ§åˆ¶APIè°ƒç”¨é—´éš”ï¼Œé˜²æ­¢è¢«å°IP
- æŒç»­è¿è¡Œï¼Œé™¤éäººå·¥å¹²é¢„é€€å‡º

ä½¿ç”¨æ–¹æ³•ï¼š
python auto_analyze_stocks.py

é…ç½®æ–‡ä»¶ï¼š
- analyzeconfig.json: ä¸»è¦é…ç½®æ–‡ä»¶ï¼ˆå¿…é¡»ï¼‰
- keys.json: å¯†é’¥æ–‡ä»¶ï¼ˆå¿…é¡»ï¼‰

é…ç½®æ–‡ä»¶æ¨¡æ¿å·²æä¾›ï¼šanalyzeconfig_template.json

ä¾èµ–ï¼š
- anaByQwen2.py: å¤ç”¨è‚¡ç¥¨åˆ†æç›¸å…³å‡½æ•°
- md_to_html.py: Markdownè½¬HTMLå·¥å…·
"""

import akshare as ak
import random
import json
import pandas as pd
from datetime import datetime, timedelta, date
from typing import List, Dict, Any, Optional
from pathlib import Path
import time as t
import sys
import signal
import traceback
import logging
from logging.handlers import RotatingFileHandler
import os

# ä»anaByQwen2.pyå¯¼å…¥éœ€è¦çš„å‡½æ•°
from anaByQwen2 import (
    load_config,
    get_and_save_stock_data,
    upload_file,
    chat_with_qwen,
    send_email,
    cleanup_stock_data,
    extract_investment_rating,
    select_prompt_by_model,
    get_kline_date_range,
    get_intraday_date_range,
    get_stock_output_dir
)
from md_to_html import MarkdownToHTMLConverter

# ===== é…ç½®å¸¸é‡ =====
DEFAULT_STOCKS_LIST_FILE = 'all_stocks_list.csv'
DEFAULT_ANALYZED_RECORDS_FILE = 'analyzed_stocks.json'
DEFAULT_LOG_FILE = 'auto_analyze_log.txt'
DEFAULT_CONFIG_FILE = 'analyzeconfig.json'
DEFAULT_KEYS_FILE = 'keys.json'

# ===== APIè°ƒç”¨é—´éš”æ§åˆ¶å™¨ =====
class APICallIntervalController:
    """
    APIè°ƒç”¨é—´éš”æ§åˆ¶å™¨
    
    åŠŸèƒ½ï¼š
    - æ­£å¸¸é—´éš”ï¼š1-2åˆ†é’Ÿéšæœº
    - è¿ç»­å¤±è´¥3æ¬¡åï¼šç­‰å¾…15åˆ†é’Ÿï¼Œé—´éš”å»¶é•¿è‡³2-4åˆ†é’Ÿ
    - æˆåŠŸä¸€æ¬¡åé‡ç½®å¤±è´¥è®¡æ•°å’Œé—´éš”
    """
    
    def __init__(self, 
                 normal_min: int = 60, 
                 normal_max: int = 120,
                 extended_min: int = 120,
                 extended_max: int = 240,
                 failure_wait_time: int = 900,
                 failure_threshold: int = 3):
        """
        åˆå§‹åŒ–é—´éš”æ§åˆ¶å™¨
        
        :param normal_min: æ­£å¸¸é—´éš”æœ€å°å€¼ï¼ˆç§’ï¼‰
        :param normal_max: æ­£å¸¸é—´éš”æœ€å¤§å€¼ï¼ˆç§’ï¼‰
        :param extended_min: å»¶é•¿é—´éš”æœ€å°å€¼ï¼ˆç§’ï¼‰
        :param extended_max: å»¶é•¿é—´éš”æœ€å¤§å€¼ï¼ˆç§’ï¼‰
        :param failure_wait_time: è¿ç»­å¤±è´¥åç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
        :param failure_threshold: è¿ç»­å¤±è´¥é˜ˆå€¼
        """
        self.normal_min = normal_min
        self.normal_max = normal_max
        self.extended_min = extended_min
        self.extended_max = extended_max
        self.failure_wait_time = failure_wait_time
        self.failure_threshold = failure_threshold
        
        self.consecutive_failures = 0
        self.is_extended_interval = False
        self.last_failure_time = None
    
    def get_next_interval(self) -> int:
        """
        è·å–ä¸‹æ¬¡APIè°ƒç”¨çš„é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
        
        :return: é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
        """
        if self.is_extended_interval:
            # å»¶é•¿é—´éš”ï¼š2-4åˆ†é’Ÿ
            interval = random.randint(self.extended_min, self.extended_max)
            print(f"â±ï¸  ä½¿ç”¨å»¶é•¿é—´éš”: {interval}ç§’ ({interval//60}åˆ†{interval%60}ç§’)")
        else:
            # æ­£å¸¸é—´éš”ï¼š1-2åˆ†é’Ÿ
            interval = random.randint(self.normal_min, self.normal_max)
            print(f"â±ï¸  ä½¿ç”¨æ­£å¸¸é—´éš”: {interval}ç§’ ({interval//60}åˆ†{interval%60}ç§’)")
        
        return interval
    
    def record_success(self):
        """è®°å½•æˆåŠŸè°ƒç”¨ï¼Œé‡ç½®å¤±è´¥è®¡æ•°å’Œé—´éš”"""
        if self.consecutive_failures > 0:
            print(f"âœ… APIè°ƒç”¨æˆåŠŸï¼Œé‡ç½®å¤±è´¥è®¡æ•°ï¼ˆä¹‹å‰è¿ç»­å¤±è´¥{self.consecutive_failures}æ¬¡ï¼‰")
        self.consecutive_failures = 0
        self.is_extended_interval = False
        self.last_failure_time = None
    
    def record_failure(self):
        """è®°å½•å¤±è´¥è°ƒç”¨ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦å»¶é•¿é—´éš”"""
        self.consecutive_failures += 1
        self.last_failure_time = datetime.now()
        
        print(f"âŒ APIè°ƒç”¨å¤±è´¥ï¼Œè¿ç»­å¤±è´¥æ¬¡æ•°: {self.consecutive_failures}/{self.failure_threshold}")
        
        if self.consecutive_failures >= self.failure_threshold:
            if not self.is_extended_interval:
                print(f"âš ï¸  è¿ç»­å¤±è´¥{self.failure_threshold}æ¬¡ï¼Œå°†å»¶é•¿é—´éš”å¹¶ç­‰å¾…{self.failure_wait_time//60}åˆ†é’Ÿ")
                self.is_extended_interval = True
                return True  # éœ€è¦ç­‰å¾…
        return False  # ä¸éœ€è¦ç­‰å¾…
    
    def wait_after_failures(self):
        """è¿ç»­å¤±è´¥åçš„ç­‰å¾…"""
        if self.consecutive_failures >= self.failure_threshold:
            print(f"â³ ç­‰å¾…{self.failure_wait_time//60}åˆ†é’Ÿåç»§ç»­...")
            for i in range(self.failure_wait_time):
                if i % 60 == 0:
                    remaining = self.failure_wait_time - i
                    print(f"   å‰©ä½™ç­‰å¾…æ—¶é—´: {remaining//60}åˆ†{remaining%60}ç§’", end='\r')
                t.sleep(1)
            print(f"\nâœ… ç­‰å¾…å®Œæˆï¼Œç»§ç»­æ‰§è¡Œï¼ˆé—´éš”å·²å»¶é•¿è‡³{self.extended_min//60}-{self.extended_max//60}åˆ†é’Ÿï¼‰")


# ===== è‚¡ç¥¨åˆ—è¡¨ç®¡ç† =====
def get_all_stocks_list(cache_file: str = DEFAULT_STOCKS_LIST_FILE) -> pd.DataFrame:
    """
    è·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨åˆ—è¡¨
    
    :param cache_file: ç¼“å­˜æ–‡ä»¶è·¯å¾„
    :return: DataFrameï¼ŒåŒ…å«æ‰€æœ‰Aè‚¡è‚¡ç¥¨ä¿¡æ¯
    """
    cache_path = Path(cache_file)
    
    # å¦‚æœç¼“å­˜æ–‡ä»¶å­˜åœ¨ï¼Œç›´æ¥è¯»å–
    if cache_path.exists():
        try:
            print(f"ğŸ“‚ ä»ç¼“å­˜æ–‡ä»¶è¯»å–è‚¡ç¥¨åˆ—è¡¨: {cache_file}")
            df = pd.read_csv(cache_file, encoding='utf-8-sig')
            print(f"âœ… æˆåŠŸè¯»å– {len(df)} åªè‚¡ç¥¨")
            return df
        except Exception as e:
            print(f"âš ï¸  è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†é‡æ–°ä¸‹è½½")
    
    # å¦‚æœç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥ï¼Œä»æ¥å£è·å–
    print("ğŸ“¥ æ­£åœ¨ä»æ¥å£è·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨åˆ—è¡¨...")
    try:
        df = ak.stock_zh_a_spot_em()
        if df is not None and not df.empty:
            # ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶
            df.to_csv(cache_file, index=False, encoding='utf-8-sig')
            print(f"âœ… æˆåŠŸè·å– {len(df)} åªè‚¡ç¥¨ï¼Œå·²ä¿å­˜åˆ°: {cache_file}")
            return df
        else:
            raise Exception("è·å–çš„è‚¡ç¥¨åˆ—è¡¨ä¸ºç©º")
    except Exception as e:
        print(f"âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
        raise


def update_stocks_list_if_needed(cache_file: str = DEFAULT_STOCKS_LIST_FILE, force_update: bool = False) -> pd.DataFrame:
    """
    å¦‚æœéœ€è¦ï¼Œæ›´æ–°è‚¡ç¥¨åˆ—è¡¨
    
    :param cache_file: ç¼“å­˜æ–‡ä»¶è·¯å¾„
    :param force_update: æ˜¯å¦å¼ºåˆ¶æ›´æ–°
    :return: DataFrameï¼ŒåŒ…å«æ‰€æœ‰Aè‚¡è‚¡ç¥¨ä¿¡æ¯
    """
    cache_path = Path(cache_file)
    
    # å¦‚æœå¼ºåˆ¶æ›´æ–°æˆ–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œé‡æ–°è·å–
    if force_update or not cache_path.exists():
        return get_all_stocks_list(cache_file)
    
    # å¦åˆ™ç›´æ¥è¯»å–ç¼“å­˜
    return get_all_stocks_list(cache_file)


# ===== å·²åˆ†æè‚¡ç¥¨è®°å½•ç®¡ç† =====
def load_analyzed_stocks(record_file: str = DEFAULT_ANALYZED_RECORDS_FILE) -> Dict[str, str]:
    """
    åŠ è½½å·²åˆ†æè‚¡ç¥¨è®°å½•
    
    :param record_file: è®°å½•æ–‡ä»¶è·¯å¾„
    :return: å­—å…¸ï¼Œæ ¼å¼ä¸º {"è‚¡ç¥¨ä»£ç ": "åˆ†ææ—¥æœŸæ—¶é—´"}
    """
    record_path = Path(record_file)
    
    if record_path.exists():
        try:
            with open(record_file, 'r', encoding='utf-8') as f:
                records = json.load(f)
            print(f"ğŸ“‹ åŠ è½½å·²åˆ†æè‚¡ç¥¨è®°å½•: {len(records)} åª")
            return records
        except Exception as e:
            print(f"âš ï¸  åŠ è½½å·²åˆ†æè®°å½•å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°è®°å½•")
    
    return {}


def save_analyzed_stocks(record_file: str, records: Dict[str, str]):
    """
    ä¿å­˜å·²åˆ†æè‚¡ç¥¨è®°å½•
    
    :param record_file: è®°å½•æ–‡ä»¶è·¯å¾„
    :param records: è®°å½•å­—å…¸
    """
    try:
        with open(record_file, 'w', encoding='utf-8') as f:
            json.dump(records, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âŒ ä¿å­˜å·²åˆ†æè®°å½•å¤±è´¥: {e}")


def is_stock_analyzed(stock_code: str, records: Dict[str, str]) -> bool:
    """
    æ£€æŸ¥è‚¡ç¥¨æ˜¯å¦å·²åˆ†æ
    
    :param stock_code: è‚¡ç¥¨ä»£ç 
    :param records: å·²åˆ†æè®°å½•å­—å…¸
    :return: Trueè¡¨ç¤ºå·²åˆ†æï¼ŒFalseè¡¨ç¤ºæœªåˆ†æ
    """
    return stock_code in records


def mark_stock_analyzed(stock_code: str, record_file: str, records: Dict[str, str], logger: logging.Logger = None):
    """
    æ ‡è®°è‚¡ç¥¨ä¸ºå·²åˆ†æ
    
    :param stock_code: è‚¡ç¥¨ä»£ç 
    :param record_file: è®°å½•æ–‡ä»¶è·¯å¾„
    :param records: å·²åˆ†æè®°å½•å­—å…¸ï¼ˆä¼šè¢«ä¿®æ”¹ï¼‰
    :param logger: æ—¥å¿—è®°å½•å™¨ï¼ˆå¯é€‰ï¼‰
    """
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    records[stock_code] = timestamp
    save_analyzed_stocks(record_file, records)
    if logger:
        logger.info(f"å·²æ ‡è®°è‚¡ç¥¨ {stock_code} ä¸ºå·²åˆ†æï¼ˆ{timestamp}ï¼‰")
    else:
        print(f"âœ… å·²æ ‡è®°è‚¡ç¥¨ {stock_code} ä¸ºå·²åˆ†æï¼ˆ{timestamp}ï¼‰")


def get_unanalyzed_stocks(all_stocks_df: pd.DataFrame, analyzed_records: Dict[str, str]) -> List[str]:
    """
    è·å–æœªåˆ†æçš„è‚¡ç¥¨åˆ—è¡¨
    
    :param all_stocks_df: æ‰€æœ‰è‚¡ç¥¨DataFrame
    :param analyzed_records: å·²åˆ†æè®°å½•å­—å…¸
    :return: æœªåˆ†æçš„è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆå­—ç¬¦ä¸²ç±»å‹ï¼‰
    """
    all_stock_codes = all_stocks_df['ä»£ç '].tolist()
    # ç¡®ä¿è‚¡ç¥¨ä»£ç è½¬æ¢ä¸ºå­—ç¬¦ä¸²ç±»å‹
    unanalyzed = [str(code) for code in all_stock_codes if not is_stock_analyzed(str(code), analyzed_records)]
    return unanalyzed


def reset_daily_records_if_needed(record_file: str, records: Dict[str, str]) -> Dict[str, str]:
    """
    å¦‚æœéœ€è¦ï¼Œæ¯æ—¥é‡ç½®å·²åˆ†æè®°å½•ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
    
    :param record_file: è®°å½•æ–‡ä»¶è·¯å¾„
    :param records: å½“å‰è®°å½•å­—å…¸
    :return: æ›´æ–°åçš„è®°å½•å­—å…¸
    """
    # æ£€æŸ¥æœ€åä¸€æ¡è®°å½•çš„æ—¶é—´
    if not records:
        return records
    
    # è·å–ä»Šå¤©çš„æ—¥æœŸ
    today = date.today().strftime('%Y-%m-%d')
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ä»Šå¤©çš„è®°å½•
    has_today_record = any(today in timestamp for timestamp in records.values())
    
    # å¦‚æœæ²¡æœ‰ä»Šå¤©çš„è®°å½•ï¼Œè¯´æ˜æ˜¯æ–°çš„ä¸€å¤©ï¼Œå¯ä»¥é€‰æ‹©é‡ç½®
    # è¿™é‡Œæš‚æ—¶ä¸å®ç°è‡ªåŠ¨é‡ç½®ï¼Œä¿ç•™å†å²è®°å½•
    # å¦‚æœéœ€è¦æ¯æ—¥é‡ç½®ï¼Œå¯ä»¥å–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
    # if not has_today_record:
    #     print("ğŸ”„ æ–°çš„ä¸€å¤©ï¼Œé‡ç½®å·²åˆ†æè®°å½•")
    #     records = {}
    #     save_analyzed_stocks(record_file, records)
    
    return records


# ===== æ—¶é—´åˆ¤æ–­ =====
def is_execution_time(start_hour: int = 17, end_hour: int = 6) -> bool:
    """
    åˆ¤æ–­å½“å‰æ—¶é—´æ˜¯å¦åœ¨æ‰§è¡Œæ—¶é—´èŒƒå›´å†…ï¼ˆ17:00-æ¬¡æ—¥6:00ï¼‰
    
    :param start_hour: å¼€å§‹æ—¶é—´ï¼ˆå°æ—¶ï¼‰
    :param end_hour: ç»“æŸæ—¶é—´ï¼ˆå°æ—¶ï¼‰
    :return: Trueè¡¨ç¤ºåœ¨æ‰§è¡Œæ—¶é—´èŒƒå›´å†…
    """
    now = datetime.now()
    current_hour = now.hour
    
    # 17:00-23:59 æˆ– 00:00-06:00
    if current_hour >= start_hour or current_hour < end_hour:
        return True
    return False


def get_time_until_execution(start_hour: int = 17) -> int:
    """
    è·å–è·ç¦»æ‰§è¡Œæ—¶é—´çš„ç§’æ•°
    
    :param start_hour: å¼€å§‹æ—¶é—´ï¼ˆå°æ—¶ï¼‰
    :return: éœ€è¦ç­‰å¾…çš„ç§’æ•°
    """
    now = datetime.now()
    current_hour = now.hour
    
    # å¦‚æœå½“å‰æ—¶é—´å·²ç»åœ¨æ‰§è¡Œæ—¶é—´èŒƒå›´å†…ï¼Œè¿”å›0
    if is_execution_time(start_hour):
        return 0
    
    # è®¡ç®—åˆ°ä¸‹ä¸€ä¸ªæ‰§è¡Œæ—¶é—´ç‚¹çš„ç§’æ•°
    if current_hour < start_hour:
        # ä»Šå¤©è¿˜æ²¡åˆ°17:00ï¼Œç­‰å¾…åˆ°ä»Šå¤©17:00
        target_time = now.replace(hour=start_hour, minute=0, second=0, microsecond=0)
    else:
        # å·²ç»è¿‡äº†17:00ä½†è¿˜æ²¡åˆ°æ¬¡æ—¥6:00ï¼Œè¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿï¼ˆå› ä¸ºis_execution_timeä¼šè¿”å›Trueï¼‰
        # ä½†ä¸ºäº†å®‰å…¨ï¼Œè®¡ç®—åˆ°æ˜å¤©17:00
        target_time = (now + timedelta(days=1)).replace(hour=start_hour, minute=0, second=0, microsecond=0)
    
    delta = target_time - now
    return int(delta.total_seconds())


def wait_until_execution_time(start_hour: int = 17, check_interval: int = 60):
    """
    ç­‰å¾…ç›´åˆ°æ‰§è¡Œæ—¶é—´
    
    :param start_hour: å¼€å§‹æ—¶é—´ï¼ˆå°æ—¶ï¼‰
    :param check_interval: æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
    """
    while not is_execution_time(start_hour):
        wait_seconds = get_time_until_execution(start_hour)
        if wait_seconds > check_interval:
            wait_seconds = check_interval
        
        print(f"â³ å½“å‰æ—¶é—´ä¸åœ¨æ‰§è¡ŒèŒƒå›´å†…ï¼Œç­‰å¾… {wait_seconds} ç§’åæ£€æŸ¥...")
        for i in range(wait_seconds):
            if i % 60 == 0:
                remaining = wait_seconds - i
                print(f"   å‰©ä½™ç­‰å¾…æ—¶é—´: {remaining}ç§’", end='\r')
            t.sleep(1)
        print()  # æ¢è¡Œ

# ===== ä¸»æ§åˆ¶å¾ªç¯ =====
def analyze_stock(stock_code: str, 
                interval_controller: APICallIntervalController,
                analyzed_records: Dict[str, str],
                record_file: str,
                config: Dict[str, Any],
                logger: logging.Logger) -> bool:
    """
    åˆ†æå•ä¸ªè‚¡ç¥¨
    
    :param stock_code: è‚¡ç¥¨ä»£ç ï¼ˆå­—ç¬¦ä¸²æˆ–æ•´æ•°ï¼Œä¼šè‡ªåŠ¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼‰
    :param interval_controller: APIè°ƒç”¨é—´éš”æ§åˆ¶å™¨
    :param analyzed_records: å·²åˆ†æè®°å½•å­—å…¸
    :param record_file: è®°å½•æ–‡ä»¶è·¯å¾„
    :param config: é…ç½®å­—å…¸
    :param logger: æ—¥å¿—è®°å½•å™¨
    :return: Trueè¡¨ç¤ºåˆ†ææˆåŠŸï¼ŒFalseè¡¨ç¤ºå¤±è´¥
    """
    # ç¡®ä¿ stock_code æ˜¯å­—ç¬¦ä¸²ç±»å‹
    stock_code = str(stock_code)
    logger.info(f"å¼€å§‹åˆ†æè‚¡ç¥¨: {stock_code}")
    
    try:
        # ä» config ä¸­è·å–æ—¥æœŸèŒƒå›´å‚æ•°ï¼ˆä½¿ç”¨getæ–¹æ³•ç¡®ä¿å®‰å…¨ï¼‰
        specified_date = config.get('specified_date', '').strip() or None
        intraday_days = config.get('intraday_days', 3)
        intraday_start_date, intraday_end_date = get_intraday_date_range(intraday_days, specified_date)
        hourly_volume_days = config.get('hourly_volume_days', intraday_days)
        hourly_start_date, hourly_end_date = get_intraday_date_range(hourly_volume_days, specified_date)
        kline_days = config.get('kline_days', 60)
        
        # åœ¨è°ƒç”¨akshareæ¥å£ä¹‹å‰ç­‰å¾…ï¼Œé¿å…APIè°ƒç”¨è¿‡å¿«è¢«å°IP
        # ä½¿ç”¨analyzeconfig.jsonä¸­çš„normal_min_intervalå’Œnormal_max_intervalä½œä¸ºç­‰å¾…æ—¶é—´ï¼ˆ1-2åˆ†é’Ÿï¼‰
        api_control = config.get('api_control', {})
        wait_min = api_control.get('normal_min_interval', 60)  # é»˜è®¤1åˆ†é’Ÿ
        wait_max = api_control.get('normal_max_interval', 120)  # é»˜è®¤2åˆ†é’Ÿ
        wait_time = random.randint(wait_min, wait_max)
        logger.info(f"â³ ç­‰å¾… {wait_time} ç§’ï¼ˆ{wait_time//60}åˆ†{wait_time%60}ç§’ï¼‰åå¼€å§‹è·å–è‚¡ç¥¨æ•°æ®ï¼Œé¿å…APIè°ƒç”¨è¿‡å¿«è¢«å°IP...")
        for i in range(wait_time):
            if i % 60 == 0 and i > 0:
                remaining = wait_time - i
                logger.info(f"   å‰©ä½™ç­‰å¾…æ—¶é—´: {remaining//60}åˆ†{remaining%60}ç§’")
            t.sleep(1)
        logger.info("âœ… ç­‰å¾…å®Œæˆï¼Œå¼€å§‹è·å–è‚¡ç¥¨æ•°æ®")
        
        # 1. ä¸‹è½½è‚¡ç¥¨æ•°æ®
        logger.info("æ­£åœ¨ä¸‹è½½è‚¡ç¥¨æ•°æ®...")
        result = get_and_save_stock_data(
            stock=stock_code,
            start_date=intraday_start_date,
            end_date=intraday_end_date,
            kline_days=kline_days,
            hourly_start_date=hourly_start_date,
            hourly_end_date=hourly_end_date
        )
        
        if result[0] is None:
            logger.error(f"è‚¡ç¥¨ {stock_code} æ•°æ®ä¸‹è½½å¤±è´¥")
            interval_controller.record_failure()
            cleanup_stock_data(stock_code)
            return False
        
        file_paths, stock_name = result
        interval_controller.record_success()
        
        # 2. ä¸Šä¼ æ–‡ä»¶å¹¶è·å– file_id
        logger.info("æ­£åœ¨ä¸Šä¼ æ–‡ä»¶...")
        main_file_path = file_paths['complete']
        api_key = config.get('qwen_api_key', config.get('api_key', ''))
        file_id = upload_file(file_path=main_file_path, api_key=api_key)
        
        if file_id is None:
            logger.error(f"è‚¡ç¥¨ {stock_code} çš„æ–‡ä»¶ä¸Šä¼ å¤±è´¥")
            cleanup_stock_data(stock_code)
            interval_controller.record_failure()
            return False
        
        # 3. å¤§æ¨¡å‹åˆ†æ
        logger.info("æ­£åœ¨è¿›è¡Œå¤§æ¨¡å‹åˆ†æ...")
        prompt_template = select_prompt_by_model(config)
        response = chat_with_qwen(
            file_id=file_id,
            question=prompt_template,
            api_key=api_key,
            intraday_days=intraday_days,
            kline_days=kline_days,
            stock_code=stock_code,
            specified_date=specified_date,
            hourly_volume_days=hourly_volume_days
        )
        
        if not response:
            logger.error(f"è‚¡ç¥¨ {stock_code} å¤§æ¨¡å‹åˆ†æå¤±è´¥")
            cleanup_stock_data(stock_code)
            return False
        
        # 4. ä¿å­˜åˆ†æç»“æœåˆ°MDæ–‡ä»¶
        logger.info("æ­£åœ¨ä¿å­˜åˆ†æç»“æœ...")
        current_time = datetime.now()
        date_str = current_time.strftime('%Y%m%d')
        time_str = current_time.strftime('%H%M%S')
        
        output_dir = get_stock_output_dir(stock_code)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        clean_stock_name = stock_name.replace('(', '').replace(')', '').replace(' ', '_')
        md_filename = f"{stock_code}_{clean_stock_name}_{intraday_start_date}_to_{intraday_end_date}_{date_str}_{time_str}.md"
        md_filepath = output_dir / md_filename
        
        with open(md_filepath, 'w', encoding='utf-8') as f:
            f.write(f"# {stock_name}ï¼ˆ{stock_code}ï¼‰è‚¡ç¥¨åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"**åˆ†ææ—¶é—´**: {current_time.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}\n\n")
            f.write(f"---\n\n")
            f.write(response)
        
        logger.info(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {md_filepath}")
        
        # 5. è½¬æ¢ä¸ºHTML
        logger.info("æ­£åœ¨è½¬æ¢ä¸ºHTML...")
        html_filename = md_filename.replace('.md', '.html')
        html_filepath = output_dir / html_filename
        converter = MarkdownToHTMLConverter()
        if not converter.convert_file(str(md_filepath), str(html_filepath)):
            logger.error(f"HTMLè½¬æ¢å¤±è´¥: {md_filepath}")
            cleanup_stock_data(stock_code)
            return False
        
        logger.info(f"HTMLæ–‡ä»¶å·²ç”Ÿæˆ: {html_filepath}")
        
        # 6. å‘é€é‚®ä»¶é€šçŸ¥
        logger.info("æ­£åœ¨å‘é€åˆ†æç»“æœé‚®ä»¶...")
        email_sender = config.get('email_sender', '')
        email_password = config.get('email_password', '')
        email_receivers = config.get('email_receivers', [])
        
        # æå–æŠ•èµ„è¯„çº§å¹¶æ·»åŠ åˆ°é‚®ä»¶ä¸»é¢˜ä¸­
        investment_rating = extract_investment_rating(str(md_filepath))
        if investment_rating:
            email_subject = f"è‚¡ç¥¨ {stock_name}ï¼ˆ{stock_code}ï¼‰åˆ†æç»“æœ - {investment_rating}"
        else:
            email_subject = f"è‚¡ç¥¨ {stock_name}ï¼ˆ{stock_code}ï¼‰åˆ†æç»“æœ"
        
        email_body = f"è‚¡ç¥¨ {stock_name}ï¼ˆ{stock_code}ï¼‰çš„åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆï¼Œè¯·æŸ¥çœ‹é™„ä»¶ä¸­çš„æ–‡ä»¶ã€‚\n\né™„ä»¶åŒ…å«ï¼š\n1. ä¸»åˆ†ææŠ¥å‘Šï¼ˆHTMLæ ¼å¼ï¼‰\n2. å°æ—¶é‡èƒ½åˆ†ææ•°æ®å·²åŒ…å«åœ¨CSVæ–‡ä»¶ä¸­"
        attachment_list = [str(html_filepath), str(md_filepath)]
        
        email_sent = send_email(
            subject=email_subject,
            body=email_body,
            receivers=email_receivers,
            sender=email_sender,
            password=email_password,
            attachment_paths=attachment_list
        )
        
        if not email_sent:
            logger.warning(f"è‚¡ç¥¨ {stock_code} é‚®ä»¶å‘é€å¤±è´¥ï¼Œä½†åˆ†æå·²å®Œæˆ")
        
        # 7. æ¸…ç†æ•°æ®ç›®å½•
        cleanup_stock_data(stock_code)
        
        # 8. æ ‡è®°ä¸ºå·²åˆ†æ
        mark_stock_analyzed(stock_code, record_file, analyzed_records)
        
        logger.info(f"è‚¡ç¥¨ {stock_code} åˆ†æå®Œæˆï¼")
        return True
        
    except Exception as e:
        logger.error(f"è‚¡ç¥¨ {stock_code} åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
        interval_controller.record_failure()
        # ç¡®ä¿åœ¨å‡ºé”™æ—¶ä¹Ÿæ¸…ç†æ•°æ®
        try:
            cleanup_stock_data(stock_code)
        except:
            pass
        return False


def setup_logging(log_file: str = DEFAULT_LOG_FILE) -> logging.Logger:
    """
    è®¾ç½®æ—¥å¿—ç³»ç»Ÿ
    
    :param log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„
    :return: æ—¥å¿—è®°å½•å™¨
    """
    # åˆ›å»ºæ—¥å¿—ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    log_dir = Path(log_file).parent
    if not log_dir.exists():
        log_dir.mkdir(parents=True, exist_ok=True)
    
    # é…ç½®æ—¥å¿—æ ¼å¼
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # åˆ›å»º rotating file handler
    file_handler = RotatingFileHandler(
        log_file,
        maxBytes=10*1024*1024,  # 10MB
        backupCount=5,
        encoding='utf-8'
    )
    file_handler.setFormatter(formatter)
    
    # åˆ›å»º console handler
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    
    # åˆ›å»º logger
    logger = logging.getLogger('auto_analyze_stocks')
    logger.setLevel(logging.INFO)
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger


def load_config_with_validation(config_file: str = DEFAULT_CONFIG_FILE, keys_file: str = DEFAULT_KEYS_FILE) -> Dict[str, Any]:
    """
    åŠ è½½å¹¶éªŒè¯é…ç½®æ–‡ä»¶
    
    :param config_file: é…ç½®æ–‡ä»¶è·¯å¾„
    :param keys_file: å¯†é’¥æ–‡ä»¶è·¯å¾„
    :return: é…ç½®å­—å…¸
    :raises: FileNotFoundError, ValueError
    """
    # åŠ è½½é…ç½®æ–‡ä»¶
    if not Path(config_file).exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ {config_file} ä¸å­˜åœ¨")
    
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
    except Exception as e:
        raise ValueError(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}")
    
    # åŠ è½½å¯†é’¥æ–‡ä»¶
    if not Path(keys_file).exists():
        raise FileNotFoundError(f"å¯†é’¥æ–‡ä»¶ {keys_file} ä¸å­˜åœ¨")
    
    try:
        with open(keys_file, 'r', encoding='utf-8') as f:
            keys = json.load(f)
    except Exception as e:
        raise ValueError(f"å¯†é’¥æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}")
    
    # éªŒè¯å¿…è¦é…ç½®é¡¹
    required_keys = ['execution_time', 'api_control', 'analysis']
    for key in required_keys:
        if key not in config:
            raise ValueError(f"é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…è¦é¡¹: {key}")
    
    # éªŒè¯æ‰§è¡Œæ—¶é—´é…ç½®
    exec_time = config['execution_time']
    if 'start_hour' not in exec_time or 'end_hour' not in exec_time:
        raise ValueError("é…ç½®æ–‡ä»¶ç¼ºå°‘æ‰§è¡Œæ—¶é—´é…ç½®")
    
    # éªŒè¯APIæ§åˆ¶é…ç½®
    api_control = config['api_control']
    required_api_keys = ['normal_min_interval', 'normal_max_interval', 
                      'extended_min_interval', 'extended_max_interval',
                      'failure_wait_time', 'failure_threshold']
    for key in required_api_keys:
        if key not in api_control:
            raise ValueError(f"é…ç½®æ–‡ä»¶ç¼ºå°‘APIæ§åˆ¶é…ç½®: {key}")
    
    # éªŒè¯åˆ†æé…ç½®
    analysis_config = config['analysis']
    required_analysis_keys = ['output_base_dir', 'max_retries', 'retry_delay', 'api_timeout']
    for key in required_analysis_keys:
        if key not in analysis_config:
            raise ValueError(f"é…ç½®æ–‡ä»¶ç¼ºå°‘åˆ†æé…ç½®: {key}")
    
    # åˆå¹¶keys.jsonä¸­çš„é…ç½®åˆ°configä¸­ï¼ˆä¸anaByQwen2.pyçš„load_configä¿æŒä¸€è‡´ï¼‰
    config.update(keys)
    
    # å°è¯•ä»anylizeconfig.jsonè¯»å–ç¼ºå¤±çš„é…ç½®é¡¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    analyze_config_file = 'anylizeconfig.json'
    if Path(analyze_config_file).exists():
        try:
            with open(analyze_config_file, 'r', encoding='utf-8') as f:
                analyze_config = json.load(f)
            # åˆå¹¶åˆ†æç›¸å…³çš„é…ç½®é¡¹
            for key in ['intraday_days', 'hourly_volume_days', 'kline_days', 'specified_date', 'prompt']:
                if key in analyze_config and key not in config:
                    config[key] = analyze_config[key]
                    print(f"âœ… ä» {analyze_config_file} è¯»å–é…ç½®é¡¹: {key}")
        except Exception as e:
            print(f"âš ï¸  è¯»å– {analyze_config_file} å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼")
    
    # è®¾ç½®é»˜è®¤å€¼ï¼ˆå¦‚æœé…ç½®ä¸­ä¸å­˜åœ¨ï¼‰
    config.setdefault('intraday_days', 3)
    config.setdefault('hourly_volume_days', 10)
    config.setdefault('kline_days', 60)
    config.setdefault('specified_date', '')
    
    return config


def main_control_loop(logger: logging.Logger, debug_mode: bool = False):
    """
    ä¸»æ§åˆ¶å¾ªç¯
    
    åŠŸèƒ½ï¼š
    - æŒç»­è¿è¡Œï¼Œç›´åˆ°äººå·¥å¹²é¢„é€€å‡º
    - åœ¨æ‰§è¡Œæ—¶é—´èŒƒå›´å†…åˆ†æè‚¡ç¥¨
    - ç®¡ç†åˆ†æè¿›åº¦å’ŒçŠ¶æ€
    
    :param logger: æ—¥å¿—è®°å½•å™¨
    :param debug_mode: è°ƒè¯•æ¨¡å¼ï¼Œå¦‚æœä¸ºTrueåˆ™è·³è¿‡æ—¶é—´æ£€æµ‹ï¼Œç«‹å³æ‰§è¡Œ
    """
    if debug_mode:
        logger.info("ğŸ› è°ƒè¯•æ¨¡å¼å·²å¯ç”¨ï¼šè·³è¿‡æ—¶é—´æ£€æµ‹ï¼Œç«‹å³æ‰§è¡Œ")
    logger.info("è‡ªåŠ¨è‚¡ç¥¨åˆ†æç³»ç»Ÿå¯åŠ¨")
    logger.info("=" * 50)
    
    try:
        # 1. åŠ è½½é…ç½®
        config = load_config_with_validation()
        logger.info("é…ç½®åŠ è½½å®Œæˆ")
        
        # 2. è·å–è‚¡ç¥¨åˆ—è¡¨
        all_stocks_df = update_stocks_list_if_needed()
        logger.info(f"å…±æœ‰ {len(all_stocks_df)} åªè‚¡ç¥¨éœ€è¦åˆ†æ")
        
        # 3. åŠ è½½å·²åˆ†æè®°å½•
        analyzed_records = load_analyzed_stocks()
        analyzed_records = reset_daily_records_if_needed(DEFAULT_ANALYZED_RECORDS_FILE, analyzed_records)
        
        # 4. åˆå§‹åŒ–APIé—´éš”æ§åˆ¶å™¨
        api_control = config['api_control']
        interval_controller = APICallIntervalController(
            normal_min=api_control['normal_min_interval'],
            normal_max=api_control['normal_max_interval'],
            extended_min=api_control['extended_min_interval'],
            extended_max=api_control['extended_max_interval'],
            failure_wait_time=api_control['failure_wait_time'],
            failure_threshold=api_control['failure_threshold']
        )
        
        # 5. ä¸»å¾ªç¯
        while True:
            # æ£€æŸ¥æ˜¯å¦åœ¨æ‰§è¡Œæ—¶é—´èŒƒå›´å†…ï¼ˆè°ƒè¯•æ¨¡å¼ä¸‹è·³è¿‡ï¼‰
            if not debug_mode and not is_execution_time():
                logger.info("å½“å‰æ—¶é—´ä¸åœ¨æ‰§è¡ŒèŒƒå›´å†…ï¼Œç­‰å¾…...")
                wait_until_execution_time()
                continue
            
            # è·å–æœªåˆ†æçš„è‚¡ç¥¨
            unanalyzed_stocks = get_unanalyzed_stocks(all_stocks_df, analyzed_records)
            
            if not unanalyzed_stocks:
                logger.info("æ‰€æœ‰è‚¡ç¥¨éƒ½å·²åˆ†æå®Œæˆï¼")
                # ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡ç½®è®°å½•ï¼ˆå¯é€‰ï¼‰
                logger.info("ç­‰å¾…24å°æ—¶åé‡ç½®åˆ†æè®°å½•...")
                t.sleep(86400)  # ç­‰å¾…24å°æ—¶
                analyzed_records = {}  # é‡ç½®è®°å½•
                save_analyzed_stocks(DEFAULT_ANALYZED_RECORDS_FILE, analyzed_records)
                continue
            
            # éšæœºé€‰æ‹©ä¸€åªè‚¡ç¥¨è¿›è¡Œåˆ†æ
            stock_to_analyze = random.choice(unanalyzed_stocks)
            logger.info(f"éšæœºé€‰æ‹©è‚¡ç¥¨: {stock_to_analyze}")
            
            # åˆ†æè‚¡ç¥¨
            analyze_stock(stock_to_analyze, interval_controller, analyzed_records, 
                        DEFAULT_ANALYZED_RECORDS_FILE, config, logger)
            
            # è·å–ä¸‹ä¸€æ¬¡APIè°ƒç”¨çš„é—´éš”æ—¶é—´
            interval = interval_controller.get_next_interval()
            
            # ç­‰å¾…æŒ‡å®šé—´éš”æ—¶é—´
            logger.info(f"ç­‰å¾… {interval} ç§’åè¿›è¡Œä¸‹ä¸€æ¬¡åˆ†æ...")
            t.sleep(interval)
            
    except KeyboardInterrupt:
        logger.info("\nç”¨æˆ·ä¸­æ–­ï¼Œç¨‹åºé€€å‡º")
    except Exception as e:
        logger.error(f"\nç¨‹åºå‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}")
        logger.error("é”™è¯¯è¯¦æƒ…:")
        logger.error(traceback.format_exc())
    finally:
        logger.info("\nç¨‹åºç»“æŸ")


# ===== ä¿¡å·å¤„ç† =====
def cleanup_resources():
    """
    æ¸…ç†ç¨‹åºèµ„æº
    """
    print("\nğŸ§¹ æ­£åœ¨æ¸…ç†ç¨‹åºèµ„æº...")
    # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šçš„èµ„æºæ¸…ç†é€»è¾‘
    print("âœ… èµ„æºæ¸…ç†å®Œæˆ")


def signal_handler(sig, frame):
    """
    ä¿¡å·å¤„ç†å‡½æ•°ï¼Œç”¨äºä¼˜é›…é€€å‡º
    """
    print("\nâ¹ï¸  æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œç¨‹åºå°†é€€å‡º...")
    cleanup_resources()
    sys.exit(0)


def setup_signal_handlers():
    """
    è®¾ç½®ä¿¡å·å¤„ç†
    """
    signal.signal(signal.SIGINT, signal_handler)  # Ctrl+C
    signal.signal(signal.SIGTERM, signal_handler)  # ç»ˆæ­¢ä¿¡å·


# ===== ä¸»å‡½æ•° =====
def main():
    """
    ä¸»å‡½æ•°
    
    å‘½ä»¤è¡Œå‚æ•°ï¼š
    - æ— å‚æ•°ï¼šæ­£å¸¸æ¨¡å¼ï¼ŒæŒ‰æ—¶é—´æ£€æµ‹æ‰§è¡Œ
    - å‚æ•°ä¸º1ï¼šè°ƒè¯•æ¨¡å¼ï¼Œè·³è¿‡æ—¶é—´æ£€æµ‹ï¼Œç«‹å³æ‰§è¡Œ
    """
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    debug_mode = False
    if len(sys.argv) > 1:
        try:
            debug_flag = int(sys.argv[1])
            if debug_flag == 1:
                debug_mode = True
                print("ğŸ› è°ƒè¯•æ¨¡å¼å·²å¯ç”¨ï¼šè·³è¿‡æ—¶é—´æ£€æµ‹ï¼Œç«‹å³æ‰§è¡Œ")
            else:
                print(f"âš ï¸  æœªçŸ¥å‚æ•°: {debug_flag}ï¼Œä½¿ç”¨æ­£å¸¸æ¨¡å¼")
        except ValueError:
            print(f"âš ï¸  æ— æ•ˆå‚æ•°: {sys.argv[1]}ï¼Œä½¿ç”¨æ­£å¸¸æ¨¡å¼")
    
    try:
        # è®¾ç½®ä¿¡å·å¤„ç†
        setup_signal_handlers()
        
        # è®¾ç½®æ—¥å¿—ç³»ç»Ÿ
        logger = setup_logging()
        
        # è¿è¡Œä¸»æ§åˆ¶å¾ªç¯
        main_control_loop(logger, debug_mode=debug_mode)
        
    except Exception as e:
        print(f"âŒ ç¨‹åºå¯åŠ¨å¤±è´¥: {str(e)}")
        print("ğŸ“ é”™è¯¯è¯¦æƒ…:")
        traceback.print_exc()
    finally:
        print("\nğŸ‘‹ ç¨‹åºç»“æŸ")


if __name__ == "__main__":
    main()