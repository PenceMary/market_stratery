import akshare as ak
import random
import json
import pandas as pd
from datetime import datetime, timedelta, date
from typing import List, Dict, Any
from openai import OpenAI
import os
from pathlib import Path
import smtplib
import sys
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.application import MIMEApplication
import time as t
from md_to_html import MarkdownToHTMLConverter
import re
from hourly_volume_analysis import analyze_csv_file
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError

# ===== é…ç½®å¸¸é‡ =====
MAX_RETRIES = 3
RETRY_DELAY = 20  # ç§’
API_TIMEOUT = 180  # ç§’
OUTPUT_BASE_DIR = 'data_output'
SMTP_SERVER = 'applesmtp.163.com'
SMTP_PORT = 465
RANDOM_WAIT_MIN = 1
RANDOM_WAIT_MAX = 20

# ===== é…ç½®ç®¡ç†å‡½æ•° =====
def get_stock_output_dir(stock: str) -> Path:
    """è·å–è‚¡ç¥¨ä¸“å±è¾“å‡ºç›®å½•"""
    return Path(OUTPUT_BASE_DIR) / stock

def get_intraday_cache_path(stock: str, date: str) -> Path:
    """è·å–åˆ†æ—¶æ•°æ®ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
    return get_stock_output_dir(stock) / f"{stock}_{date}_intraday.csv"

# ===== é€šç”¨APIè°ƒç”¨åŒ…è£…å™¨ =====
def fetch_with_timeout(api_func, *args, **kwargs):
    """
    é€šç”¨çš„APIè°ƒç”¨åŒ…è£…å™¨ï¼Œå¸¦è¶…æ—¶å’Œé‡è¯•æœºåˆ¶
    
    :param api_func: APIå‡½æ•°
    :param args: ä½ç½®å‚æ•°
    :param kwargs: å…³é”®å­—å‚æ•°
    :return: è·å–åˆ°çš„æ•°æ®
    """
    with ThreadPoolExecutor(max_workers=1) as executor:
        future = executor.submit(api_func, *args, **kwargs)
        try:
            return future.result(timeout=API_TIMEOUT)
        except FutureTimeoutError:
            raise TimeoutError("API call timed out")

def fetch_with_retry(api_func, *args, **kwargs):
    """
    å¸¦é‡è¯•æœºåˆ¶çš„APIè°ƒç”¨åŒ…è£…å™¨
    
    :param api_func: APIå‡½æ•°
    :param args: ä½ç½®å‚æ•°
    :param kwargs: å…³é”®å­—å‚æ•°
    :return: è·å–åˆ°çš„æ•°æ®
    """
    max_retries = MAX_RETRIES
    retry_delay = RETRY_DELAY
    
    for attempt in range(max_retries):
        try:
            print(f"æ­£åœ¨è°ƒç”¨API... (å°è¯• {attempt + 1}/{max_retries})")
            result = fetch_with_timeout(api_func, *args, **kwargs)
            print("APIè°ƒç”¨æˆåŠŸ")
            return result
        except Exception as e:
            print(f"APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
            if attempt < max_retries - 1:
                print(f"å°†åœ¨ {retry_delay} ç§’åé‡è¯•...")
                t.sleep(retry_delay)
            else:
                raise Exception(f"APIè°ƒç”¨å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡: {str(e)}")

def extract_investment_rating(md_file_path: str) -> str:
    """
    ä»MDæ–‡ä»¶ä¸­æå–æŠ•èµ„è¯„çº§ä¿¡æ¯

    :param md_file_path: str, MDæ–‡ä»¶è·¯å¾„
    :return: str, æå–åˆ°çš„æŠ•èµ„è¯„çº§ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    try:
        with open(md_file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾æŠ•èµ„è¯„çº§è¡Œ
        # åŒ¹é…ç±»ä¼¼ï¼š**æŠ•èµ„è¯„çº§** | âœ… **å¼ºçƒˆæ¨èï¼ˆStrong Buyï¼‰** |
        pattern = r'\*\*æŠ•èµ„è¯„çº§\*\*\s*\|\s*(.+?)\s*\|'
        match = re.search(pattern, content)

        if match:
            rating_text = match.group(1).strip()
            # æ¸…ç†markdownæ ¼å¼ï¼Œæå–å®é™…è¯„çº§å†…å®¹
            # ç§»é™¤markdownçš„ç²—ä½“æ ‡è®°å’Œè¡¨æƒ…ç¬¦å·
            clean_rating = re.sub(r'\*\*', '', rating_text)  # ç§»é™¤ç²—ä½“æ ‡è®°
            clean_rating = re.sub(r'[âœ…âŒğŸŸ¢ğŸŸ¡ğŸ”´]', '', clean_rating)  # ç§»é™¤è¡¨æƒ…ç¬¦å·
            clean_rating = clean_rating.strip()
            return clean_rating
        else:
            print(f"âš ï¸ åœ¨æ–‡ä»¶ {md_file_path} ä¸­æœªæ‰¾åˆ°æŠ•èµ„è¯„çº§ä¿¡æ¯")
            return ""

    except Exception as e:
        print(f"âŒ æå–æŠ•èµ„è¯„çº§æ—¶å‡ºé”™: {e}")
        return ""

def send_email(subject: str, body: str, receivers: List[str], sender: str, password: str, attachment_paths: List[str] = None) -> bool:
    """å‘é€é‚®ä»¶å¹¶è¿”å›æ˜¯å¦æˆåŠŸï¼Œå¦‚æœæä¾›attachment_pathsåˆ™å‘é€å¤šä¸ªé™„ä»¶"""
    # åˆ›å»ºé‚®ä»¶å¯¹è±¡
    msg = MIMEMultipart()
    msg['From'] = sender  # å‘ä»¶äºº
    msg['To'] = ', '.join(receivers)  # å°†æ”¶ä»¶äººåˆ—è¡¨è½¬æ¢ä¸ºé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
    msg['Subject'] = subject

    # æ·»åŠ é‚®ä»¶æ­£æ–‡
    msg.attach(MIMEText(body, 'plain', 'utf-8'))

    # å¦‚æœæä¾›äº†é™„ä»¶è·¯å¾„åˆ—è¡¨ï¼Œæ·»åŠ æ‰€æœ‰é™„ä»¶
    if attachment_paths:
        for attachment_path in attachment_paths:
            if attachment_path and os.path.exists(attachment_path):
                try:
                    with open(attachment_path, 'rb') as f:
                        # æ ¹æ®æ–‡ä»¶æ‰©å±•åç¡®å®šMIMEç±»å‹
                        file_ext = os.path.splitext(attachment_path)[1].lower()
                        if file_ext == '.html':
                            attachment = MIMEApplication(f.read(), _subtype='html')
                        elif file_ext == '.md':
                            attachment = MIMEApplication(f.read(), _subtype='text')
                        else:
                            attachment = MIMEApplication(f.read())
                        
                        attachment.add_header('Content-Disposition', 'attachment', filename=os.path.basename(attachment_path))
                        msg.attach(attachment)
                    print(f"å·²æ·»åŠ é™„ä»¶: {attachment_path}")
                except Exception as e:
                    print(f"æ·»åŠ é™„ä»¶å¤±è´¥: {attachment_path}, é”™è¯¯: {e}")
                    continue  # ç»§ç»­æ·»åŠ å…¶ä»–é™„ä»¶
            else:
                print(f"é™„ä»¶æ–‡ä»¶ä¸å­˜åœ¨: {attachment_path}")

    # SMTPæœåŠ¡å™¨è®¾ç½®
    smtp_server = SMTP_SERVER
    smtp_port = SMTP_PORT

    # ç™»å½•å‡­è¯ï¼ˆä½¿ç”¨æˆæƒç ï¼‰
    username = sender

    # å‘é€é‚®ä»¶
    try:
        server = smtplib.SMTP_SSL(smtp_server, smtp_port)
        server.login(username, password)
        server.sendmail(sender, receivers, msg.as_string())
        server.quit()
        print("é‚®ä»¶å‘é€æˆåŠŸï¼")
        # å¦‚æœé‚®ä»¶å‘é€æˆåŠŸä¸”æä¾›äº†é™„ä»¶è·¯å¾„ï¼Œåˆ™åˆ é™¤æœ¬åœ°æ–‡ä»¶
        if attachment_paths:
            for attachment_path in attachment_paths:
                if attachment_path and os.path.exists(attachment_path):
                    #os.remove(attachment_path)
                    print(f"æœ¬åœ°æ–‡ä»¶ {attachment_path} å·²åˆ é™¤ï¼ˆä¸ºæ–¹ä¾¿è°ƒè¯•ï¼Œç›®å‰éœ€æ‰‹åŠ¨åˆ é™¤ï¼‰")
        return True
    except Exception as e:
        print(f"é‚®ä»¶å‘é€å¤±è´¥ï¼š{e}")
        return False

def load_config(config_file: str, keys_file: str) -> Dict[str, Any]:
    """è¯»å– JSON é…ç½®æ–‡ä»¶å¹¶è¿”å›é…ç½®å­—å…¸"""
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        with open(keys_file, 'r', encoding='utf-8') as f:
            keys = json.load(f)
        config.update(keys)  # åˆå¹¶ keys.json ä¸­çš„é…ç½®
        return config
    except Exception as e:
        raise Exception(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

def select_stocks(config: Dict[str, Any]) -> List[str]:
    """æ ¹æ®é…ç½®æ–‡ä»¶é€‰æ‹©è‚¡ç¥¨"""
    if config['stock_selection'] == 'specified':
        return config['specified_stocks']
    elif config['stock_selection'] == 'random':
        all_stocks = ak.stock_zh_a_spot_em()['ä»£ç '].tolist()
        return random.sample(all_stocks, min(config['random_stock_count'], len(all_stocks)))
    else:
        raise ValueError("é…ç½®æ–‡ä»¶ä¸­çš„ 'stock_selection' å¿…é¡»æ˜¯ 'specified' æˆ– 'random'")

def get_trading_dates(start_date: str, end_date: str) -> List[str]:
    """
    è·å–æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„äº¤æ˜“æ—¥åˆ—è¡¨ã€‚
    
    :param start_date: str, èµ·å§‹æ—¥æœŸï¼Œæ ¼å¼ 'YYYYMMDD'
    :param end_date: str, ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ 'YYYYMMDD'
    :return: list, äº¤æ˜“æ—¥åˆ—è¡¨ï¼Œæ ¼å¼ä¸º 'YYYYMMDD'
    """
    calendar = ak.tool_trade_date_hist_sina()
    calendar['trade_date'] = pd.to_datetime(calendar['trade_date'])
    start_date_dt = pd.to_datetime(start_date)
    end_date_dt = pd.to_datetime(end_date)
    trading_dates = calendar[(calendar['trade_date'] >= start_date_dt) & 
                             (calendar['trade_date'] <= end_date_dt)]['trade_date']
    return trading_dates.dt.strftime('%Y%m%d').tolist()

def get_intraday_data(stock: str, start_date: str, end_date: str) -> pd.DataFrame:
    """
    è·å–æŒ‡å®šè‚¡ç¥¨åœ¨æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„åˆ†æ—¶æˆäº¤æ•°æ®ã€‚
    
    :param stock: str, è‚¡ç¥¨ä»£ç ï¼Œä¾‹å¦‚ '600000'
    :param start_date: str, èµ·å§‹æ—¥æœŸï¼Œæ ¼å¼ 'YYYYMMDD'
    :param end_date: str, ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ 'YYYYMMDD'
    :return: pd.DataFrame, åˆ†æ—¶æˆäº¤æ•°æ®ï¼Œsymbol å’Œ name åˆ—å·²åˆ é™¤
    """
    # æ ¹æ®è‚¡ç¥¨ä»£ç å‰ç¼€è°ƒæ•´åˆ†é’Ÿçº¿ä»£ç 
    if stock.startswith('688'):
        minute_code = f"sh{stock}"
    elif stock.startswith(('83', '43', '87')):
        minute_code = f"bj{stock}"
    elif stock.startswith('60'):
        minute_code = f"sh{stock}"
    elif stock.startswith(('00', '30')):
        minute_code = f"sz{stock}"
    else:
        minute_code = stock
    print(f"minute_code: {minute_code}")

    trading_dates = get_trading_dates(start_date, end_date)
    stock_data_list = []
    stock_name = None
    output_dir = get_stock_output_dir(stock)
    output_dir.mkdir(parents=True, exist_ok=True)


    for date in trading_dates:
        local_path = get_intraday_cache_path(stock, date)
        max_retries = MAX_RETRIES  # æœ€å¤§é‡è¯•æ¬¡æ•°
        loaded_from_local = False

        if local_path.exists():
            try:
                daily_data = pd.read_csv(local_path, encoding='utf-8-sig')
                # Handle potential timezone in ticktime
                daily_data['ticktime'] = pd.to_datetime(daily_data['ticktime'], utc=True).dt.tz_localize(None)
                print(f"ä»æœ¬åœ°åŠ è½½ {minute_code} åœ¨ {date} çš„æ•°æ®")
                loaded_from_local = True
            except Exception as e:
                print(f"åŠ è½½æœ¬åœ°æ–‡ä»¶ {local_path} å¤±è´¥: {e}ï¼Œå°†ä»æ¥å£é‡æ–°è·å–")

        if not loaded_from_local:
            try:
                daily_data = fetch_with_retry(ak.stock_intraday_sina, symbol=minute_code, date=date)
                if not daily_data.empty:
                    daily_data['ticktime'] = pd.to_datetime(date + ' ' + daily_data['ticktime'])
                    daily_data.to_csv(local_path, index=False, encoding='utf-8-sig')
                    print(f"æˆåŠŸè·å–å¹¶ä¿å­˜ {minute_code} åœ¨ {date} çš„æ•°æ®åˆ° {local_path}")
                    # å¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œç­‰å¾…éšæœºæ—¶é—´
                    if date != trading_dates[-1]:
                        print("ç¨ç­‰ä¸€ä¸‹...")
                        for _ in range(random.randint(RANDOM_WAIT_MIN, RANDOM_WAIT_MAX)):
                            print("+", end="", flush=True)
                            t.sleep(1)
                        print()  # æ¢è¡Œ
            except Exception as e:
                print(f"è·å–è‚¡ç¥¨ {minute_code} åœ¨ {date} çš„æ•°æ®å¤±è´¥: {e}")
                continue

        if 'daily_data' in locals() and not daily_data.empty:
            # è·å– stock_nameï¼Œå¦‚æœå°šæœªè®¾ç½®
            if stock_name is None and 'name' in daily_data.columns:
                stock_name = daily_data['name'][0]
            stock_data_list.append(daily_data)

    if not stock_data_list:
        raise ValueError(f"æ— æ³•è·å– {minute_code} çš„é€ç¬”æˆäº¤æ•°æ®")

    # å¦‚æœ stock_name ä»ä¸º Noneï¼ˆæ‰€æœ‰æ•°æ®ä»æœ¬åœ°åŠ è½½ï¼Œä¸”æœ¬åœ°æ—  name åˆ—ï¼‰ï¼Œåˆ™ä»æ¥å£è·å–ä¸€ä¸ªæ—¥æœŸçš„ name
    if stock_name is None:
        try:
            # ä½¿ç”¨æœ€åä¸€ä¸ªäº¤æ˜“æ—¥è·å– name
            sample_data = ak.stock_intraday_sina(symbol=minute_code, date=trading_dates[-1])
            if not sample_data.empty:
                stock_name = sample_data['name'][0]
                print(f"ä»æ¥å£è·å–è‚¡ç¥¨åç§°: {stock_name}")
        except Exception as e:
            print(f"æ— æ³•è·å–è‚¡ç¥¨åç§°: {e}")
            stock_name = "æœªçŸ¥"  # é»˜è®¤å€¼

    all_data = pd.concat(stock_data_list)
    all_data = all_data.sort_values('ticktime').reset_index(drop=True)
    all_data = all_data.drop(columns=['symbol', 'name'], errors='ignore')
    return all_data, stock_name

def get_daily_kline_data(symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
    """
    è·å–æŒ‡å®šè‚¡ç¥¨åœ¨æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ—¥Kçº¿æ•°æ®ã€‚

    :param symbol: str, è‚¡ç¥¨ä»£ç ï¼Œä¾‹å¦‚ '300680'
    :param start_date: str, èµ·å§‹æ—¥æœŸï¼Œæ ¼å¼ 'YYYYMMDD'
    :param end_date: str, ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ 'YYYYMMDD'
    :return: pd.DataFrame, æ—¥Kçº¿æ•°æ®
    """
    # è·å–æ—¥Kçº¿æ•°æ®ï¼Œå¸¦é‡è¯•æœºåˆ¶
    try:
        stock_data = fetch_with_retry(ak.stock_zh_a_hist, symbol=symbol, period="daily", start_date=start_date, end_date=end_date, adjust="")
        if stock_data is not None and not stock_data.empty:
            print(f"æˆåŠŸè·å–è‚¡ç¥¨ {symbol} çš„Kçº¿æ•°æ®ï¼Œå…± {len(stock_data)} æ¡è®°å½•")
            return stock_data
        else:
            print(f"è­¦å‘Šï¼šè‚¡ç¥¨ {symbol} çš„Kçº¿æ•°æ®ä¸ºç©º")
            return pd.DataFrame()
    except Exception as e:
        raise Exception(f"è·å–è‚¡ç¥¨ {symbol} çš„Kçº¿æ•°æ®å¤±è´¥: {str(e)}")

def _fetch_index_data_with_retry(api_func, *args, **kwargs):
    """
    å¸¦é‡è¯•æœºåˆ¶çš„æŒ‡æ•°æ•°æ®è·å–å‡½æ•°

    :param api_func: APIå‡½æ•°
    :param args: ä½ç½®å‚æ•°
    :param kwargs: å…³é”®å­—å‚æ•°
    :return: è·å–åˆ°çš„æ•°æ®
    """
    return fetch_with_retry(api_func, *args, **kwargs)

def get_market_index_data(stock_code: str, start_date: str, end_date: str) -> dict:
    """
    æ ¹æ®è‚¡ç¥¨ä»£ç è·å–å¯¹åº”çš„å¤§ç›˜æŒ‡æ•°æ—¥Kçº¿æ•°æ®ï¼Œæ”¯æŒå¤šä¸ªæŒ‡æ•°ï¼ˆä¸»æ¿æŒ‡æ•°+æ¿å—æŒ‡æ•°ï¼‰ã€‚

    :param stock_code: str, è‚¡ç¥¨ä»£ç ï¼Œä¾‹å¦‚ '600000'
    :param start_date: str, èµ·å§‹æ—¥æœŸï¼Œæ ¼å¼ 'YYYYMMDD'
    :param end_date: str, ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ 'YYYYMMDD'
    :return: dict, {æŒ‡æ•°åç§°: (pd.DataFrame, æŒ‡æ•°å…¨ç§°)} - å¤šä¸ªæŒ‡æ•°çš„æ•°æ®å­—å…¸
    """
    print(f"æ­£åœ¨è·å–è‚¡ç¥¨ {stock_code} å¯¹åº”çš„å¤§ç›˜æŒ‡æ•°æ•°æ®...")

    # æ ¹æ®è‚¡ç¥¨ä»£ç ç¡®å®šéœ€è¦è·å–çš„æŒ‡æ•°åˆ—è¡¨
    index_configs = {}

    if stock_code.startswith('60'):
        # ä¸Šæµ·ä¸»æ¿ï¼šä¸Šè¯æŒ‡æ•°
        index_configs = {
            "ä¸Šè¯æŒ‡æ•°": ("000001", "ä¸Šè¯æŒ‡æ•°")
        }
        print("è¯†åˆ«ä¸ºä¸Šæµ·ä¸»æ¿è‚¡ç¥¨ï¼Œä½¿ç”¨ä¸Šè¯æŒ‡æ•°")
    elif stock_code.startswith('688'):
        # ç§‘åˆ›æ¿ï¼šä¸Šè¯æŒ‡æ•° + ç§‘åˆ›50
        index_configs = {
            "ä¸Šè¯æŒ‡æ•°": ("000001", "ä¸Šè¯æŒ‡æ•°"),
            "ç§‘åˆ›50": ("000688", "ç§‘åˆ›50æŒ‡æ•°")
        }
        print("è¯†åˆ«ä¸ºç§‘åˆ›æ¿è‚¡ç¥¨ï¼Œä½¿ç”¨ä¸Šè¯æŒ‡æ•°å’Œç§‘åˆ›50æŒ‡æ•°")
    elif stock_code.startswith('00'):
        # æ·±åœ³ä¸»æ¿ï¼šæ·±åœ³æˆæŒ‡
        index_configs = {
            "æ·±åœ³æˆæŒ‡": ("399001", "æ·±åœ³æˆæŒ‡")
        }
        print("è¯†åˆ«ä¸ºæ·±åœ³ä¸»æ¿è‚¡ç¥¨ï¼Œä½¿ç”¨æ·±åœ³æˆæŒ‡")
    elif stock_code.startswith('30'):
        # åˆ›ä¸šæ¿ï¼šæ·±åœ³æˆæŒ‡ + åˆ›ä¸šæ¿æŒ‡æ•°
        index_configs = {
            "æ·±åœ³æˆæŒ‡": ("399001", "æ·±åœ³æˆæŒ‡"),
            "åˆ›ä¸šæ¿æŒ‡æ•°": ("399006", "åˆ›ä¸šæ¿æŒ‡æ•°")
        }
        print("è¯†åˆ«ä¸ºåˆ›ä¸šæ¿è‚¡ç¥¨ï¼Œä½¿ç”¨æ·±åœ³æˆæŒ‡å’Œåˆ›ä¸šæ¿æŒ‡æ•°")
    elif stock_code.startswith(('83', '43', '87')):
        # åŒ—äº¤æ‰€ï¼šåŒ—è¯50
        index_configs = {
            "åŒ—è¯50": ("899050", "åŒ—è¯50æŒ‡æ•°")
        }
        print("è¯†åˆ«ä¸ºåŒ—äº¤æ‰€è‚¡ç¥¨ï¼Œä½¿ç”¨åŒ—è¯50æŒ‡æ•°")
    else:
        # é»˜è®¤ä½¿ç”¨ä¸Šè¯æŒ‡æ•°
        index_configs = {
            "ä¸Šè¯æŒ‡æ•°": ("000001", "ä¸Šè¯æŒ‡æ•°")
        }
        print("æ— æ³•è¯†åˆ«å¸‚åœºç±»å‹ï¼Œé»˜è®¤ä½¿ç”¨ä¸Šè¯æŒ‡æ•°")

    result_data = {}

    for short_name, (index_code, full_name) in index_configs.items():
        try:
            # ç‰¹æ®Šå¤„ç†ä¸Šè¯æŒ‡æ•°ï¼Œé¿å…ä¸å¹³å®‰é“¶è¡Œä»£ç å†²çª
            if index_code == "000001":
                # ä½¿ç”¨æŒ‡æ•°ä¸“ç”¨APIè·å–ä¸Šè¯æŒ‡æ•°æ•°æ®ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
                index_data = _fetch_index_data_with_retry(ak.stock_zh_index_daily, symbol="sh000001")
                # ç­›é€‰æŒ‡å®šæ—¥æœŸèŒƒå›´çš„æ•°æ®
                index_data['date'] = pd.to_datetime(index_data['date'])
                index_data = index_data[(index_data['date'] >= pd.to_datetime(start_date)) & (index_data['date'] <= pd.to_datetime(end_date))]
                # å°†è‹±æ–‡åˆ—åè½¬æ¢ä¸ºä¸­æ–‡åˆ—åï¼Œä¸å…¶ä»–æŒ‡æ•°ä¿æŒä¸€è‡´
                index_data = index_data.rename(columns={
                    'date': 'æ—¥æœŸ',
                    'open': 'å¼€ç›˜',
                    'high': 'æœ€é«˜',
                    'low': 'æœ€ä½',
                    'close': 'æ”¶ç›˜',
                    'volume': 'æˆäº¤é‡'
                })
                print(f"âœ… ä½¿ç”¨æŒ‡æ•°ä¸“ç”¨APIè·å– {full_name} æ•°æ®æˆåŠŸ")
            else:
                # å…¶ä»–æŒ‡æ•°ä½¿ç”¨åŸæœ‰çš„æ–¹æ³•ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
                index_data = _fetch_index_data_with_retry(
                    ak.stock_zh_a_hist,
                    symbol=index_code,
                    period="daily",
                    start_date=start_date,
                    end_date=end_date,
                    adjust=""
                )

            if index_data.empty:
                print(f"âŒ è·å– {full_name} æ•°æ®å¤±è´¥ï¼Œè·³è¿‡")
                continue

            print(f"âœ… {full_name} æ•°æ®è·å–æˆåŠŸï¼Œå…± {len(index_data)} æ¡è®°å½•")
            if not index_data.empty:
                print(f"   æ—¶é—´èŒƒå›´: {index_data['æ—¥æœŸ'].min()} åˆ° {index_data['æ—¥æœŸ'].max()}")

            result_data[short_name] = (index_data, full_name)

        except Exception as e:
            print(f"âŒ è·å– {full_name} æ•°æ®æ—¶å‡ºé”™: {e}")
            continue

    if not result_data:
        print("âŒ æœªèƒ½è·å–ä»»ä½•æŒ‡æ•°æ•°æ®")
        return {"æœªçŸ¥æŒ‡æ•°": (pd.DataFrame(), "æœªçŸ¥æŒ‡æ•°")}

    return result_data

def get_industry_sector_data(stock_code: str, start_date: str, end_date: str) -> tuple:
    """
    è·å–è‚¡ç¥¨æ‰€å±è¡Œä¸šæ¿å—çš„æ—¥Kçº¿æ•°æ®å’Œæ¿å—åç§°ã€‚

    :param stock_code: str, è‚¡ç¥¨ä»£ç ï¼Œä¾‹å¦‚ '600000'
    :param start_date: str, èµ·å§‹æ—¥æœŸï¼Œæ ¼å¼ 'YYYYMMDD'
    :param end_date: str, ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ 'YYYYMMDD'
    :return: tuple, (pd.DataFrame, str) - è¡Œä¸šæ¿å—æ—¥Kçº¿æ•°æ®å’Œæ¿å—åç§°
    """
    print(f"æ­£åœ¨è·å–è‚¡ç¥¨ {stock_code} æ‰€å±è¡Œä¸šæ¿å—æ•°æ®...")

    def fetch_with_timeout(func, *args, **kwargs):
        with ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(func, *args, **kwargs)
            try:
                return future.result(timeout=API_TIMEOUT)
            except FutureTimeoutError:
                raise TimeoutError("API call timed out")

    try:
        # æ­¥éª¤1: è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        stock_info_df = _fetch_index_data_with_retry(fetch_with_timeout, ak.stock_individual_info_em, symbol=stock_code)

        if stock_info_df.empty:
            print("âŒ è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯å¤±è´¥")
            return pd.DataFrame(), "æœªçŸ¥æ¿å—"

        # ä»DataFrameä¸­æå–ä¿¡æ¯
        info_dict = dict(zip(stock_info_df['item'], stock_info_df['value']))

        stock_name = info_dict.get('è‚¡ç¥¨ç®€ç§°', 'æœªçŸ¥')
        industry_name = info_dict.get('è¡Œä¸š', 'æœªçŸ¥')

        print(f"âœ… è‚¡ç¥¨ä¿¡æ¯è·å–æˆåŠŸ: {stock_name}")
        print(f"   è¡Œä¸šåˆ†ç±»: {industry_name}")

        if industry_name == 'æœªçŸ¥' or not industry_name:
            print("âŒ æ— æ³•è·å–è¡Œä¸šåˆ†ç±»ä¿¡æ¯")
            return pd.DataFrame(), "æœªçŸ¥æ¿å—"

        # æ­¥éª¤2: è·å–è¡Œä¸šæ¿å—æ•°æ®ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        print(f"æ­£åœ¨è·å– '{industry_name}' è¡Œä¸šæ¿å—æ•°æ®...")
        industry_data = _fetch_index_data_with_retry(
            fetch_with_timeout,
            ak.stock_board_industry_hist_em,
            symbol=industry_name,
            start_date=start_date,
            end_date=end_date
        )

        if industry_data.empty:
            print(f"âŒ è·å– '{industry_name}' è¡Œä¸šæ¿å—æ•°æ®å¤±è´¥")
            print("   å¯èƒ½åŸå› : è¡Œä¸šåç§°æ ¼å¼ä¸åŒ¹é…æˆ–æ•°æ®ä¸å¯ç”¨")
            return pd.DataFrame(), f"{industry_name}(æ•°æ®è·å–å¤±è´¥)"

        print(f"âœ… è¡Œä¸šæ¿å—æ•°æ®è·å–æˆåŠŸï¼Œå…± {len(industry_data)} æ¡è®°å½•")
        print(f"   æ—¶é—´èŒƒå›´: {industry_data['æ—¥æœŸ'].min()} åˆ° {industry_data['æ—¥æœŸ'].max()}")

        return industry_data, industry_name

    except Exception as e:
        print(f"âŒ è·å–è¡Œä¸šæ¿å—æ•°æ®æ—¶å‡ºé”™: {e}")
        return pd.DataFrame(), "æœªçŸ¥æ¿å—"

# ===== æ•°æ®è·å–æ¨¡å— =====
def fetch_all_stock_data(stock: str, start_date: str, end_date: str, kline_days: int) -> tuple:
    """
    è·å–è‚¡ç¥¨çš„æ‰€æœ‰ç›¸å…³æ•°æ®
    
    :param stock: str, è‚¡ç¥¨ä»£ç 
    :param start_date: str, åˆ†æ—¶æ•°æ®çš„èµ·å§‹æ—¥æœŸ
    :param end_date: str, åˆ†æ—¶æ•°æ®çš„ç»“æŸæ—¥æœŸ
    :param kline_days: int, æ—¥Kçº¿æ•°æ®çš„å¤©æ•°
    :return: tuple, (df_intraday, stock_name, df_daily, market_index_data, df_industry, industry_sector_name)
    """
    # åˆ†æ—¶æ•°æ®ä½¿ç”¨ä¼ é€’çš„æ—¥æœŸèŒƒå›´
    df_intraday, stock_name = get_intraday_data(stock=stock, start_date=start_date, end_date=end_date)

    # Kçº¿æ•°æ®ä½¿ç”¨åŸºäºkline_daysè®¡ç®—çš„æ—¥æœŸèŒƒå›´
    kline_start_date, kline_end_date = get_kline_date_range(kline_days, end_date)
    df_daily = get_daily_kline_data(symbol=stock, start_date=kline_start_date, end_date=kline_end_date)

    # å¤§ç›˜æŒ‡æ•°æ•°æ®ä½¿ç”¨Kçº¿æ•°æ®çš„æ—¥æœŸèŒƒå›´
    market_index_data = get_market_index_data(stock_code=stock, start_date=kline_start_date, end_date=kline_end_date)

    # è¡Œä¸šæ¿å—æ•°æ®ä½¿ç”¨Kçº¿æ•°æ®çš„æ—¥æœŸèŒƒå›´
    df_industry, industry_sector_name = get_industry_sector_data(stock_code=stock, start_date=kline_start_date, end_date=kline_end_date)
    
    return df_intraday, stock_name, df_daily, market_index_data, df_industry, industry_sector_name

def create_complete_csv_file(stock: str, stock_name: str, start_date: str, end_date: str, 
                           kline_start_date: str, kline_end_date: str, industry_sector_name: str,
                           df_intraday: pd.DataFrame, df_daily: pd.DataFrame, 
                           market_index_data: dict, df_industry: pd.DataFrame, 
                           hourly_start_date: str = None, hourly_end_date: str = None) -> str:
    """
    åˆ›å»ºåŒ…å«æ‰€æœ‰æ•°æ®çš„å®Œæ•´CSVæ–‡ä»¶
    
    :param stock: str, è‚¡ç¥¨ä»£ç 
    :param stock_name: str, è‚¡ç¥¨åç§°
    :param start_date: str, åˆ†æ—¶æ•°æ®èµ·å§‹æ—¥æœŸ
    :param end_date: str, åˆ†æ—¶æ•°æ®ç»“æŸæ—¥æœŸ
    :param kline_start_date: str, Kçº¿æ•°æ®èµ·å§‹æ—¥æœŸ
    :param kline_end_date: str, Kçº¿æ•°æ®ç»“æŸæ—¥æœŸ
    :param industry_sector_name: str, è¡Œä¸šæ¿å—åç§°
    :param df_intraday: pd.DataFrame, åˆ†æ—¶æ•°æ®
    :param df_daily: pd.DataFrame, æ—¥Kçº¿æ•°æ®
    :param market_index_data: dict, å¤§ç›˜æŒ‡æ•°æ•°æ®
    :param df_industry: pd.DataFrame, è¡Œä¸šæ¿å—æ•°æ®
    :return: str, æ–‡ä»¶è·¯å¾„
    """
    # ç”Ÿæˆä¸‰ä½éšæœºæ•°ï¼Œé¿å…æ–‡ä»¶åå†²çª
    random_suffix = str(random.randint(0, 999)).zfill(3)
    base_filename = f"{stock}_{stock_name}_{start_date}_to_{end_date}_{random_suffix}"
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = get_stock_output_dir(stock)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºå®Œæ•´CSVæ–‡ä»¶
    main_file = str(output_dir / f"{base_filename}_complete.csv")
    with open(main_file, 'w', encoding='utf-8-sig', newline='') as f:
        # å†™å…¥æ ‡é¢˜ä¿¡æ¯
        f.write(f"è‚¡ç¥¨ä»£ç : {stock}\n")
        f.write(f"è‚¡ç¥¨åç§°: {stock_name}\n")
        f.write(f"æ‰€å±æ¿å—: {industry_sector_name}\n")
        f.write(f"åˆ†æ—¶æ•°æ®æ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}\n")
        f.write(f"Kçº¿æ•°æ®æ—¶é—´èŒƒå›´: {kline_start_date} åˆ° {kline_end_date}\n")
        # ä½¿ç”¨å°æ—¶é‡èƒ½ä¸“ç”¨æ—¶é—´èŒƒå›´ï¼Œå¦‚æœæ²¡æœ‰æä¾›åˆ™ä½¿ç”¨åˆ†æ—¶æ•°æ®æ—¶é—´èŒƒå›´
        hourly_range_start = hourly_start_date if hourly_start_date else start_date
        hourly_range_end = hourly_end_date if hourly_end_date else end_date
        f.write(f"å°æ—¶é‡èƒ½æ•°æ®æ—¶é—´èŒƒå›´: {hourly_range_start} åˆ° {hourly_range_end}\n\n")

        # å†™å…¥åˆ†æ—¶æ•°æ®
        f.write("=== åˆ†æ—¶æˆäº¤æ•°æ® ===\n")
        df_intraday.to_csv(f, index=False)
        f.write("\n\n")

        # å†™å…¥æ—¥Kçº¿æ•°æ®
        f.write("=== æ—¥Kçº¿æ•°æ® ===\n")
        df_daily.to_csv(f, index=False)
        f.write("\n\n")

        # å†™å…¥å¤§ç›˜æŒ‡æ•°æ•°æ®
        f.write("=== å¤§ç›˜æŒ‡æ•°æ•°æ® ===\n")
        for index_name, (index_df, index_full_name) in market_index_data.items():
            f.write(f"--- {index_full_name} ---\n")
            index_df.to_csv(f, index=False)
            f.write("\n")

        # å†™å…¥è¡Œä¸šæ¿å—æ•°æ®
        f.write("=== è¡Œä¸šæ¿å—æ•°æ® ===\n")
        df_industry.to_csv(f, index=False)
        f.write("\n\n")

    return main_file

def get_and_save_stock_data(stock: str, start_date: str, end_date: str, kline_days: int, hourly_start_date: str = None, hourly_end_date: str = None) -> tuple:
    """
    è·å–è‚¡ç¥¨çš„åˆ†æ—¶æˆäº¤æ•°æ®ã€æ—¥Kçº¿æ•°æ®ã€å¤§ç›˜æŒ‡æ•°æ•°æ®å’Œè¡Œä¸šæ¿å—æ•°æ®ï¼Œå¹¶ä¿å­˜åˆ°CSVæ–‡ä»¶ä¸­ã€‚

    :param stock: str, è‚¡ç¥¨ä»£ç ï¼Œä¾‹å¦‚ '300680'
    :param start_date: str, åˆ†æ—¶æ•°æ®çš„èµ·å§‹æ—¥æœŸï¼Œæ ¼å¼ 'YYYYMMDD'
    :param end_date: str, åˆ†æ—¶æ•°æ®çš„ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ 'YYYYMMDD'
    :param kline_days: int, æ—¥Kçº¿æ•°æ®çš„å¤©æ•°ï¼Œä¾‹å¦‚ 60
    :param hourly_start_date: str, å°æ—¶é‡èƒ½åˆ†æçš„èµ·å§‹æ—¥æœŸï¼Œæ ¼å¼ 'YYYYMMDD'ï¼Œå¯é€‰
    :param hourly_end_date: str, å°æ—¶é‡èƒ½åˆ†æçš„ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ 'YYYYMMDD'ï¼Œå¯é€‰
    :return: tuple, (file_paths, stock_name) æ–‡ä»¶è·¯å¾„å­—å…¸å’Œè‚¡ç¥¨åç§°ï¼Œå¤±è´¥è¿”å› (None, None)
    """
    try:
        # åˆ†æ—¶æ•°æ®ä½¿ç”¨ä¼ é€’çš„æ—¥æœŸèŒƒå›´ï¼ˆæ¥è‡ªdaysBeforeTodayï¼‰
        df_intraday, stock_name = get_intraday_data(stock=stock, start_date=start_date, end_date=end_date)

        # Kçº¿æ•°æ®ä½¿ç”¨åŸºäºkline_daysè®¡ç®—çš„æ—¥æœŸèŒƒå›´ï¼Œä½¿ç”¨ä¸åˆ†æ—¶æ•°æ®ç›¸åŒçš„ç»“æŸæ—¥æœŸ
        kline_start_date, kline_end_date = get_kline_date_range(kline_days, end_date)
        df_daily = get_daily_kline_data(symbol=stock, start_date=kline_start_date, end_date=kline_end_date)

        # å¤§ç›˜æŒ‡æ•°æ•°æ®ä½¿ç”¨Kçº¿æ•°æ®çš„æ—¥æœŸèŒƒå›´
        market_index_data = get_market_index_data(stock_code=stock, start_date=kline_start_date, end_date=kline_end_date)

        # è¡Œä¸šæ¿å—æ•°æ®ä½¿ç”¨Kçº¿æ•°æ®çš„æ—¥æœŸèŒƒå›´
        df_industry, industry_sector_name = get_industry_sector_data(stock_code=stock, start_date=kline_start_date, end_date=kline_end_date)

        # åˆ›å»ºå®Œæ•´CSVæ–‡ä»¶
        main_file = create_complete_csv_file(
            stock, stock_name, start_date, end_date, kline_start_date, kline_end_date,
            industry_sector_name, df_intraday, df_daily, market_index_data, df_industry,
            hourly_start_date, hourly_end_date
        )
        
        file_paths = {'complete': main_file}
        print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {main_file}")

        # è°ƒç”¨å°æ—¶é‡èƒ½åˆ†æå¹¶æ’å…¥ç»“æœ
        print(f"ğŸ” å¼€å§‹å¯¹ {stock} è¿›è¡Œå°æ—¶é‡èƒ½åˆ†æ...")
        try:
            # å¦‚æœæä¾›äº†å°æ—¶é‡èƒ½ä¸“ç”¨æ—¥æœŸèŒƒå›´ï¼Œåˆ™ä½¿ç”¨ä¸“ç”¨èŒƒå›´è¿›è¡Œåˆ†æ
            if hourly_start_date and hourly_end_date:
                print(f"ğŸ“Š ä½¿ç”¨å°æ—¶é‡èƒ½ä¸“ç”¨æ—¥æœŸèŒƒå›´: {hourly_start_date} åˆ° {hourly_end_date}")
                # è·å–å°æ—¶é‡èƒ½ä¸“ç”¨çš„åˆ†æ—¶æ•°æ®
                df_hourly_intraday, _ = get_intraday_data(stock=stock, start_date=hourly_start_date, end_date=hourly_end_date)
                # åˆ›å»ºä¸´æ—¶CSVæ–‡ä»¶ç”¨äºå°æ—¶é‡èƒ½åˆ†æ
                output_dir = get_stock_output_dir(stock)
                base_filename = f"{stock}_{stock_name}_{start_date}_to_{end_date}_{str(random.randint(0, 999)).zfill(3)}"
                temp_hourly_file = str(output_dir / f"{base_filename}_hourly_temp.csv")
                with open(temp_hourly_file, 'w', encoding='utf-8-sig', newline='') as f_temp:
                    f_temp.write(f"è‚¡ç¥¨ä»£ç : {stock}\n")
                    f_temp.write(f"è‚¡ç¥¨åç§°: {stock_name}\n")
                    f_temp.write(f"å°æ—¶é‡èƒ½æ•°æ®æ—¶é—´èŒƒå›´: {hourly_start_date} åˆ° {hourly_end_date}\n\n")
                    f_temp.write("=== åˆ†æ—¶æˆäº¤æ•°æ® ===\n")
                    df_hourly_intraday.to_csv(f_temp, index=False)
                    f_temp.write("\n\n")
                    f_temp.write("=== æ—¥Kçº¿æ•°æ® ===\n")  # æ·»åŠ ç»“æŸæ ‡è®°
                hourly_analysis_result, hourly_md_path = analyze_csv_file(temp_hourly_file)
                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                os.remove(temp_hourly_file)
            else:
                # ä½¿ç”¨åŸæœ‰çš„å®Œæ•´æ–‡ä»¶è¿›è¡Œåˆ†æï¼ˆå‘åå…¼å®¹ï¼‰
                hourly_analysis_result, hourly_md_path = analyze_csv_file(main_file)
            if hourly_analysis_result is not None and hourly_md_path is not None:
                print(f"âœ… å°æ—¶é‡èƒ½åˆ†æå®Œæˆ")
                # åˆ é™¤MDæ–‡ä»¶ï¼Œå› ä¸ºæ•°æ®å·²åŒ…å«åœ¨CSVä¸­
                try:
                    os.remove(hourly_md_path)
                    print(f"ğŸ—‘ï¸ å·²åˆ é™¤ä¸´æ—¶MDæ–‡ä»¶: {hourly_md_path}")
                except Exception as e:
                    print(f"âš ï¸ åˆ é™¤MDæ–‡ä»¶å¤±è´¥: {e}")
                
                # å°†å°æ—¶é‡èƒ½åˆ†ææ•°æ®è¿½åŠ åˆ°ä¸»CSVæ–‡ä»¶
                with open(main_file, 'a', encoding='utf-8-sig', newline='') as f_append:
                    f_append.write("=== å°æ—¶é‡èƒ½åˆ†ææ•°æ® ===\n")
                    f_append.write("æ—¥æœŸ,æ—¶é—´æ®µ,æ€»ç¬”æ•°,æˆäº¤é‡,æ€»é‡èƒ½,Uå æ¯”,Då æ¯”,Eå æ¯”,U/D,æˆäº¤é‡å æ¯”\n")
                    
                    for date in sorted(hourly_analysis_result.keys()):
                        period_stats = hourly_analysis_result[date]
                        daily_stats = []
                        
                        # è®¡ç®—å½“å¤©çš„æˆäº¤é‡æ€»å’Œï¼ˆåŒ…å«æ‰€æœ‰æ—¶é—´æ®µï¼ŒåŒ…æ‹¬09:25ï¼‰
                        daily_total_volume_count = 0
                        for period_name, stats in period_stats.items():
                            daily_total_volume_count += stats['total_volume_count']
                        
                        # å†™å…¥æ¯ä¸ªæ—¶é—´æ®µ
                        for period_name, stats in period_stats.items():
                            ud_display = stats['ud_ratio'] if stats['ud_ratio'] != 'NA' else 'NA'
                            # è®¡ç®—æˆäº¤é‡å æ¯”
                            if daily_total_volume_count > 0:
                                volume_ratio = stats['total_volume_count'] / daily_total_volume_count
                            else:
                                volume_ratio = 0
                            f_append.write(f"{date},{stats['period_name']},{stats['transaction_count']},{stats['total_volume_count']:.0f},{stats['total_volume']:.0f},{stats['u_ratio']:.4f},{stats['d_ratio']:.4f},{stats['e_ratio']:.4f},{ud_display},{volume_ratio:.4f}\n")
                            daily_stats.append(stats)
                        
                        # è®¡ç®—å¹¶å†™å…¥æ¯å¤©æ±‡æ€»
                        if daily_stats:
                            # æ’é™¤09:25æ—¶é—´æ®µï¼Œåªè®¡ç®—09:30-15:00çš„æ±‡æ€»æ•°æ®
                            filtered_stats = [s for s in daily_stats if s.get('period_name') != '09:25']
                            
                            if filtered_stats:
                                total_transactions = sum(s['transaction_count'] for s in filtered_stats)
                                total_volume = sum(s['total_volume'] for s in filtered_stats)
                                total_volume_count = sum(s['total_volume_count'] for s in filtered_stats)
                                total_u_volume = sum(s['u_volume'] for s in filtered_stats)
                                total_d_volume = sum(s['d_volume'] for s in filtered_stats)
                                total_e_volume = sum(s['e_volume'] for s in filtered_stats)
                                
                                u_ratio = total_u_volume / total_volume if total_volume > 0 else 0
                                d_ratio = total_d_volume / total_volume if total_volume > 0 else 0
                                e_ratio = total_e_volume / total_volume if total_volume > 0 else 0
                                ud_ratio = total_u_volume / total_d_volume if total_d_volume > 0 else (total_u_volume if total_u_volume > 0 else 0)
                                
                                # è®¡ç®—09:30-15:00çš„æˆäº¤é‡å æ¯”ï¼ˆæ’é™¤09:25ï¼‰
                                filtered_volume_count = sum(s['total_volume_count'] for s in filtered_stats)
                                filtered_volume_ratio = filtered_volume_count / daily_total_volume_count if daily_total_volume_count > 0 else 0
                                f_append.write(f"{date},09:30-15:00,{total_transactions},{total_volume_count:.0f},{total_volume:.0f},{u_ratio:.4f},{d_ratio:.4f},{e_ratio:.4f},{ud_ratio:.2f},{filtered_volume_ratio:.4f}\n")
                    
                    f_append.write("\n\n")
            else:
                print(f"âš ï¸ è‚¡ç¥¨ {stock} çš„å°æ—¶é‡èƒ½åˆ†æå¤±è´¥")
        except Exception as e:
            print(f"âŒ è‚¡ç¥¨ {stock} çš„å°æ—¶é‡èƒ½åˆ†æå‡ºé”™: {e}")

        return file_paths, stock_name

    except Exception as e:
        print(f"âŒ å¤„ç†è‚¡ç¥¨ {stock} æ—¶å‡ºé”™: {e}")
        return None, None

def upload_file(file_path: str, api_key: str) -> str:
    """
    ä¸Šä¼ æ–‡ä»¶åˆ°é€šä¹‰åƒé—®å¹³å°ã€‚
    
    :param file_path: str, æ–‡ä»¶è·¯å¾„
    :param api_key: str, API å¯†é’¥
    :return: str, æ–‡ä»¶ ID
    """
    client = OpenAI(
        api_key=api_key,
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
    )
    file_object = client.files.create(file=Path(file_path), purpose="file-extract")
    print(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œæ–‡ä»¶ ID: {file_object.id}")
    return file_object.id

def save_data_to_file(data_text: str, stock_code: str, file_suffix: str = "") -> str:
    """
    å°†æ ¼å¼åŒ–çš„æ•°æ®ä¿å­˜åˆ°æ–‡ä»¶ä¸­ã€‚

    :param data_text: str, æ ¼å¼åŒ–çš„æ•°æ®æ–‡æœ¬
    :param stock_code: str, è‚¡ç¥¨ä»£ç 
    :param file_suffix: str, æ–‡ä»¶åç¼€ï¼Œç”¨äºåŒºåˆ†ä¸åŒç‰ˆæœ¬
    :return: str, ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    import os
    from datetime import datetime

    # åˆ›å»ºæ•°æ®ç›®å½•
    data_dir = f"data_output/{stock_code}"
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)

    # ç”Ÿæˆæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{data_dir}/{stock_code}_data_{timestamp}{file_suffix}.txt"
    filepath = filename

    # ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(f"=== å‘é€ç»™å¤§æ¨¡å‹çš„æ•°æ® ===\n")
        f.write(f"è‚¡ç¥¨ä»£ç : {stock_code}\n")
        f.write(f"ä¿å­˜æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"=" * 50 + "\n\n")
        f.write(data_text)

    print(f"ğŸ“„ æ•°æ®å·²ä¿å­˜åˆ°æ–‡ä»¶: {filepath}")
    return filepath

def chat_with_qwen(file_id: str, question: Any, api_key: str, intraday_days: int = 7, kline_days: int = 30, stock_code: str = "", specified_date: str = None, hourly_volume_days: int = None) -> str:
    """
    ä½¿ç”¨é€šä¹‰åƒé—®çš„ API è¿›è¡ŒèŠå¤©ï¼Œæ”¯æŒå­—å…¸æˆ–å­—ç¬¦ä¸²ç±»å‹çš„ questionã€‚

    :param file_id: str, æ–‡ä»¶ ID
    :param question: Any, ç”¨æˆ·æç¤ºæˆ–é—®é¢˜ï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—å…¸
    :param api_key: str, API å¯†é’¥
    :param intraday_days: int, åˆ†æ—¶æ•°æ®çš„å¤©æ•°ï¼Œé»˜è®¤7å¤©
    :param kline_days: int, Kçº¿æ•°æ®çš„å¤©æ•°ï¼Œé»˜è®¤30å¤©
    :param stock_code: str, è‚¡ç¥¨ä»£ç ï¼Œé»˜è®¤ç©ºå­—ç¬¦ä¸²
    :param specified_date: str, æŒ‡å®šçš„æ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰ï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨ç³»ç»Ÿæ—¶é—´
    :param hourly_volume_days: int, å°æ—¶é‡èƒ½æ•°æ®çš„å¤©æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨intraday_days
    :return: str, èŠå¤©ç»“æœ
    """
    client = OpenAI(
        api_key=api_key,
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
    )

    # å¤„ç†hourly_volume_dayså‚æ•°ï¼Œå¦‚æœæ²¡æœ‰æä¾›åˆ™ä½¿ç”¨intraday_days
    if hourly_volume_days is None:
        hourly_volume_days = intraday_days

    # åˆå§‹åŒ– messages åˆ—è¡¨
    messages = [
        {'role': 'system', 'content': 'You are a helpful assistant.'},
        {'role': 'system', 'content': f'fileid://{file_id}'}
    ]

    # å¤„ç† question å‚æ•°
    if isinstance(question, dict):
        # å¦‚æœ question æ˜¯å­—å…¸ï¼Œå‡è®¾å®ƒåŒ…å« analysis_request
        analysis_request = question.get('analysis_request', {})

        # ä½¿ç”¨ä¼ å…¥çš„å‚æ•°ï¼Œä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„å‚æ•°ï¼Œå…¶æ¬¡ä»questionä¸­è·å–
        intraday_days = intraday_days
        kline_days = kline_days

        # è·å–å½“å‰æ—¶é—´ï¼šå¦‚æœæŒ‡å®šäº†æ—¥æœŸåˆ™ä½¿ç”¨æŒ‡å®šæ—¥æœŸï¼Œå¦åˆ™ä½¿ç”¨ç³»ç»Ÿæ—¶é—´
        if specified_date:
            # ä½¿ç”¨æŒ‡å®šçš„æ—¥æœŸ
            current_datetime = datetime.strptime(specified_date, '%Y%m%d')
        else:
            # ä½¿ç”¨ç³»ç»Ÿæ—¶é—´
            current_datetime = datetime.now()
        current_date_str = current_datetime.strftime('%Yå¹´%mæœˆ%dæ—¥')
        current_weekday = current_datetime.strftime('%A')  # è‹±æ–‡æ˜ŸæœŸ
        # è½¬æ¢ä¸ºä¸­æ–‡æ˜ŸæœŸ
        weekday_map = {
            'Monday': 'æ˜ŸæœŸä¸€', 'Tuesday': 'æ˜ŸæœŸäºŒ', 'Wednesday': 'æ˜ŸæœŸä¸‰',
            'Thursday': 'æ˜ŸæœŸå››', 'Friday': 'æ˜ŸæœŸäº”', 'Saturday': 'æ˜ŸæœŸå…­', 'Sunday': 'æ˜ŸæœŸæ—¥'
        }
        current_weekday_cn = weekday_map.get(current_weekday, current_weekday)

        # åœ¨æç¤ºè¯å¼€å¤´æ˜ç¡®å£°æ˜å½“å‰æ—¶é—´
        time_declaration = f"""âš ï¸ é‡è¦æ—¶é—´å£°æ˜ï¼šå½“å‰ç³»ç»Ÿæ—¶é—´ä¸º {current_date_str} {current_weekday_cn}ã€‚è¯·åœ¨æ•´ä¸ªåˆ†ææŠ¥å‘Šä¸­ä½¿ç”¨æ­¤æ—¶é—´ä½œä¸ºåŸºå‡†ï¼Œç¡®ä¿æ‰€æœ‰æ—¥æœŸç›¸å…³çš„å†…å®¹éƒ½åŸºäºæ­¤å½“å‰æ—¶é—´è¿›è¡Œè®¡ç®—å’Œæè¿°ã€‚

"""

        # æ„é€ ç”¨æˆ·æ¶ˆæ¯å†…å®¹ - å¢å¼ºçš„åˆ†ææè¿°
        user_content = time_declaration + (
            f"{analysis_request.get('analysis_purpose', {}).get('description', '')}\n\n"
            f"ğŸ“Š csvæ–‡ä»¶ä¸­çš„æ•°æ®å’Œæ—¶é—´èŒƒå›´è¯´æ˜ï¼š\n"
            f"- åˆ†æ—¶æˆäº¤æ•°æ®ï¼šåŒ…å«æœ€è¿‘ {intraday_days} ä¸ªäº¤æ˜“æ—¥çš„æ—¥å†…åˆ†æ—¶æ•°æ®ï¼Œç”¨äºåˆ†æçŸ­æœŸèµ„é‡‘æµå‘å’Œä¸»åŠ›è¡Œä¸ºæ¨¡å¼\n"
            f"- æ—¥Kçº¿æ•°æ®ï¼šåŒ…å«æœ€è¿‘ {kline_days} ä¸ªäº¤æ˜“æ—¥çš„Kçº¿æ•°æ®ï¼Œç”¨äºè¯†åˆ«ä¸­é•¿æœŸè¶‹åŠ¿å’Œå…³é”®æŠ€æœ¯ä½\n"
            f"- å¸‚åœºæŒ‡æ•°æ•°æ®ï¼šå¯¹åº”è‚¡ç¥¨æ‰€å±å¸‚åœºçš„æŒ‡æ•°ï¼ˆå¯èƒ½åŒ…å«å¤šä¸ªæŒ‡æ•°ï¼Œæ ¹æ®æ¿å—è‡ªåŠ¨åŒ¹é…ï¼‰ï¼Œæ—¶é—´èŒƒå›´ä¸Kçº¿æ•°æ®ä¸€è‡´ï¼Œç”¨äºè¯„ä¼°ç³»ç»Ÿæ€§é£é™©å’Œå¸‚åœºbetaç³»æ•°\n"
            f"- è¡Œä¸šæ¿å—æ•°æ®ï¼šè‚¡ç¥¨æ‰€å±è¡Œä¸šçš„æ¿å—æŒ‡æ•°ï¼Œæ—¶é—´èŒƒå›´ä¸Kçº¿æ•°æ®ä¸€è‡´ï¼Œç”¨äºåˆ†æè¡Œä¸šç›¸å¯¹å¼ºåº¦å’Œè½®åŠ¨æœºä¼š\n"
            f"- å°æ—¶é‡èƒ½æ•°æ®ï¼šåŸºäºæœ€è¿‘ {hourly_volume_days} ä¸ªäº¤æ˜“æ—¥çš„æ—¥å†…åˆ†æ—¶æ•°æ®è¿›è¡ŒæŒ‰å°æ—¶çº§åˆ«ç»Ÿè®¡ï¼ˆé¿å…æ•°æ®é‡è¿‡å¤§ï¼‰ï¼Œç”¨äºåˆ†æä¸­æœŸèµ„é‡‘æµå‘å’Œä¸»åŠ›è¡Œä¸ºæ¨¡å¼\n\n"
            f"ğŸ“‹ æ•°æ®ç»“æ„è¯´æ˜ï¼š\n"
            f"{analysis_request.get('data_description', {}).get('data_structure', '')}\n\n"
            f"ğŸ” æ•°æ®å·¥ä½œè¡¨è¯¦ç»†è¯´æ˜ï¼š\n"
            f"â€¢ åˆ†æ—¶æˆäº¤æ•°æ®æ®µ: {analysis_request.get('data_description', {}).get('intraday_data_section', {}).get('description', '')}\n"
            f"  æ ‡è¯†ç¬¦: {analysis_request.get('data_description', {}).get('intraday_data_section', {}).get('section_marker', '')}\n"
            f"  å­—æ®µ: {', '.join(analysis_request.get('data_description', {}).get('intraday_data_section', {}).get('fields', []))}\n"
            f"  åˆ†æé‡ç‚¹: {', '.join(analysis_request.get('data_description', {}).get('intraday_data_section', {}).get('analysis_focus', []))}\n\n"
            f"â€¢ æ—¥Kçº¿æ•°æ®æ®µ: {analysis_request.get('data_description', {}).get('daily_data_section', {}).get('description', '')}\n"
            f"  æ ‡è¯†ç¬¦: {analysis_request.get('data_description', {}).get('daily_data_section', {}).get('section_marker', '')}\n"
            f"  å­—æ®µ: {', '.join(analysis_request.get('data_description', {}).get('daily_data_section', {}).get('fields', []))}\n"
            f"  åˆ†æé‡ç‚¹: {', '.join(analysis_request.get('data_description', {}).get('daily_data_section', {}).get('analysis_focus', []))}\n\n"
            f"â€¢ å¸‚åœºæŒ‡æ•°æ•°æ®æ®µ: {analysis_request.get('data_description', {}).get('market_index_data_sections', {}).get('description', '')}\n"
            f"  æ ‡è¯†ç¬¦: {analysis_request.get('data_description', {}).get('market_index_data_sections', {}).get('section_markers', '')}\n"
            f"  å­—æ®µ: {', '.join(analysis_request.get('data_description', {}).get('market_index_data_sections', {}).get('fields', []))}\n"
            f"  åˆ†æé‡ç‚¹: {', '.join(analysis_request.get('data_description', {}).get('market_index_data_sections', {}).get('analysis_focus', []))}\n\n"
            f"â€¢ è¡Œä¸šæ¿å—æ•°æ®æ®µ: {analysis_request.get('data_description', {}).get('industry_sector_data_section', {}).get('description', '')}\n"
            f"  æ ‡è¯†ç¬¦: {analysis_request.get('data_description', {}).get('industry_sector_data_section', {}).get('section_marker', '')}\n"
            f"  å­—æ®µ: {', '.join(analysis_request.get('data_description', {}).get('industry_sector_data_section', {}).get('fields', []))}\n"
            f"  åˆ†æé‡ç‚¹: {', '.join(analysis_request.get('data_description', {}).get('industry_sector_data_section', {}).get('analysis_focus', []))}\n\n"
            f"â€¢ å°æ—¶é‡èƒ½åˆ†ææ•°æ®æ®µ: {analysis_request.get('data_description', {}).get('hourly_volume_analysis_section', {}).get('description', '')}\n"
            f"  æ ‡è¯†ç¬¦: {analysis_request.get('data_description', {}).get('hourly_volume_analysis_section', {}).get('section_marker', '')}\n"
            f"  å­—æ®µ: {', '.join(analysis_request.get('data_description', {}).get('hourly_volume_analysis_section', {}).get('fields', []))}\n"
            f"  åˆ†æé‡ç‚¹: {', '.join(analysis_request.get('data_description', {}).get('hourly_volume_analysis_section', {}).get('analysis_focus', []))}\n\n"
            f"ğŸ“ˆ å¤šæ—¥æ•°æ®åˆ†æè¦æ±‚ï¼š\n"
            f"è¯·å¯¹æä¾›çš„å¤šæ—¥åˆ†æ—¶æ•°æ®è¿›è¡Œé€æ—¥æ·±åº¦åˆ†æï¼ˆæŒ‰æ—¶é—´é¡ºåºç”±è¿œåŠè¿‘ï¼‰\n"
            f"ğŸ”¬ åˆ†ææ­¥éª¤ï¼ˆåº”ç”¨äºæ¯ä¸€å¤©çš„åˆ†æ—¶æ•°æ®åˆ†æç»“æœè¾“å‡ºï¼‰ï¼š\n"
        )

        # æ·»åŠ åˆ†ææ­¥éª¤ - é’ˆå¯¹å¤šæ—¥æ•°æ®è¿›è¡Œé€æ—¥åˆ†æ
        for step in analysis_request.get('analysis_steps', []):
            user_content += f"æ­¥éª¤ {step.get('step', '')}: {step.get('description', '')}\n"
            if step.get('output_focus'):
                user_content += f"  è¾“å‡ºé‡ç‚¹: {step.get('output_focus', '')}\n"

        # ä½¿ç”¨é…ç½®åŒ–çš„è¾“å‡ºè¦æ±‚æ ¼å¼åŒ–
        output_requirements = analysis_request.get('output_requirements', [])
        user_content += format_output_requirements(output_requirements)

        messages.append({'role': 'user', 'content': user_content})
    elif isinstance(question, str):
        # å¦‚æœ question æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨ï¼ˆä¿æŒåå‘å…¼å®¹æ€§ï¼‰
        messages.append({'role': 'user', 'content': question})
    else:
        raise ValueError("question å‚æ•°å¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ–å­—å…¸ç±»å‹")

    print(messages)

    # ä¿å­˜å®Œæ•´çš„å¯¹è¯å†…å®¹åˆ°æœ¬åœ°æ–‡ä»¶
    if stock_code:
        # å°†messagesæ ¼å¼åŒ–ä¸ºå¯è¯»çš„æ–‡æœ¬
        full_message_content = ""
        for msg in messages:
            role = msg.get('role', 'unknown')
            content = msg.get('content', '')
            full_message_content += f"=== {role.upper()} ===\n{content}\n\n"

        # ä¿å­˜åˆ°æ–‡ä»¶
        full_message_file = save_data_to_file(full_message_content, stock_code, "_full_message")
        print(f"ğŸ“„ å®Œæ•´æ¶ˆæ¯å·²ä¿å­˜ï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹: {full_message_file}")

    # è°ƒç”¨ API
    completion = client.chat.completions.create(
        model="qwen-long",
        messages=messages,
        stream=True,
        stream_options={"include_usage": True}
    )

    full_content = ""
    for chunk in completion:
        if chunk.choices and chunk.choices[0].delta.content:
            full_content += chunk.choices[0].delta.content
            print(".", end="", flush=True)

    return full_content

def format_output_requirements(output_requirements: List[Dict[str, Any]]) -> str:
    """
    æ ¹æ®output_requirementsé…ç½®æ ¼å¼åŒ–è¾“å‡ºè¦æ±‚ã€‚

    :param output_requirements: è¾“å‡ºè¦æ±‚åˆ—è¡¨
    :return: æ ¼å¼åŒ–çš„è¾“å‡ºè¦æ±‚å­—ç¬¦ä¸²
    """
    formatted_content = "\nğŸ“‹ è¾“å‡ºè¦æ±‚ï¼ˆåŸºäºå¤šæ—¥æ•°æ®åˆ†æï¼‰ï¼š\n"

    for req in output_requirements:
        section_num = req.get('section', '')
        title = req.get('title', '')
        description = req.get('description', '')

        # æ·»åŠ sectionæ ‡é¢˜å’Œæè¿°
        formatted_content += f"{section_num}. {title}: {description}\n"

        # å¤„ç†quantitative_metricsï¼ˆé‡åŒ–æŒ‡æ ‡ï¼‰
        quantitative_metrics = req.get('quantitative_metrics', [])
        if quantitative_metrics:
            formatted_content += "\né‡åŒ–æŒ‡æ ‡è¦æ±‚ï¼š\n"
            for i, metric in enumerate(quantitative_metrics, 1):
                formatted_content += f"{i}. {metric}\n"

        # å¤„ç†decision_frameworkï¼ˆå†³ç­–æ¡†æ¶ï¼‰
        decision_framework = req.get('decision_framework', {})
        if decision_framework:
            formatted_content += "\nå†³ç­–æ¡†æ¶ï¼š\n"
            for key, value in decision_framework.items():
                formatted_content += f"{key}: {value}\n"

        # å¤„ç†output_format
        output_format = req.get('output_format', {})
        if output_format:
            formatted_content += "\nè¾“å‡ºæ ¼å¼è¦æ±‚ï¼š\n"
            # é€šç”¨å¤„ç†æ‰€æœ‰output_formatä¸­çš„é”®å€¼å¯¹
            for key, value in output_format.items():
                formatted_content += f"{key}: {value}\n"

        formatted_content += "\n"

    return formatted_content

def select_prompt_by_model(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ ¹æ®å½“å‰è„šæœ¬ä½¿ç”¨çš„æ¨¡å‹æ™ºèƒ½é€‰æ‹©å¯¹åº”çš„prompté…ç½®ã€‚

    :param config: é…ç½®å­—å…¸
    :return: å¯¹åº”çš„prompté…ç½®
    """
    # å¯¹äºqwen-longï¼Œä½¿ç”¨åŸæœ‰çš„é€šç”¨promptï¼ˆåŒ…å«åˆ†æ—¶æ•°æ®å¤„ç†ï¼‰
    if 'prompt' in config:
        print("ğŸ¯ æ£€æµ‹åˆ°æ–‡ä»¶å¤„ç†æ¨¡å‹ä¸“ç”¨prompté…ç½®")
        return config['prompt']

    # å›é€€åˆ°é€šç”¨prompt
    print("â„¹ï¸ æœªæ‰¾åˆ°ä¸“ç”¨promptï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    return {}

def get_kline_date_range(kline_days: int, end_date: str = None) -> tuple:
    """
    æ ¹æ®kline_daysè®¡ç®—Kçº¿æ•°æ®çš„æ—¥æœŸèŒƒå›´ã€‚

    :param kline_days: int, Kçº¿æ•°æ®çš„å¤©æ•°
    :param end_date: str, ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYYMMDDï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ä»Šå¤©æ—¥æœŸ
    :return: tuple, (start_date, end_date) æ ¼å¼ä¸ºYYYYMMDD
    """
    if end_date is None:
        end_date = date.today().strftime('%Y%m%d')

    # è®¡ç®—Kçº¿æ•°æ®çš„å¼€å§‹æ—¥æœŸï¼ˆå¾€å‰kline_daysä¸ªäº¤æ˜“æ—¥ï¼‰
    calendar = ak.tool_trade_date_hist_sina()
    calendar['trade_date'] = pd.to_datetime(calendar['trade_date'])
    end_dt = pd.to_datetime(end_date)

    # è·å–æ‰€æœ‰äº¤æ˜“æ—¥ <= end_dtï¼Œé™åºæ’åºï¼Œå–å‰ kline_days ä¸ªï¼ˆæœ€æ–°çš„ï¼‰
    trading_dates_filtered = calendar[calendar['trade_date'] <= end_dt]['trade_date'].sort_values(ascending=False).head(kline_days)

    if len(trading_dates_filtered) < kline_days:
        print(f"âš ï¸ è­¦å‘Š: ä»…æ‰¾åˆ° {len(trading_dates_filtered)} ä¸ªäº¤æ˜“æ—¥ï¼Œå¯ç”¨äº¤æ˜“æ—¥ä¸è¶³ {kline_days} å¤©")

    start_dt_kline = trading_dates_filtered.iloc[-1]  # æœ€æ—©çš„æ—¥æœŸåœ¨æœ€åé¢ï¼Œå› ä¸ºæ˜¯é™åº
    start_date = start_dt_kline.strftime('%Y%m%d')

    return start_date, end_date

def get_intraday_date_range(days_before_today: int, end_date: str = None) -> tuple:
    """
    æ ¹æ®days_before_todayè®¡ç®—åˆ†æ—¶æ•°æ®çš„æ—¥æœŸèŒƒå›´ï¼Œä½¿ç”¨äº¤æ˜“æ—¥è€Œéè‡ªç„¶æ—¥ã€‚

    :param days_before_today: int, åˆ†æ—¶æ•°æ®å¾€å‰è¿½æº¯çš„äº¤æ˜“æ—¥æ•°é‡
    :param end_date: str, ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYYMMDDï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ä»Šå¤©æ—¥æœŸ
    :return: tuple, (start_date, end_date) æ ¼å¼ä¸ºYYYYMMDD
    """
    if end_date is None:
        end_date = date.today().strftime('%Y%m%d')

    # è®¡ç®—åˆ†æ—¶æ•°æ®çš„å¼€å§‹æ—¥æœŸï¼ˆå¾€å‰days_before_todayä¸ªäº¤æ˜“æ—¥ï¼‰
    calendar = ak.tool_trade_date_hist_sina()
    calendar['trade_date'] = pd.to_datetime(calendar['trade_date'])
    end_dt = pd.to_datetime(end_date)

    # è·å–æ‰€æœ‰äº¤æ˜“æ—¥ <= end_dtï¼Œé™åºæ’åºï¼Œå–å‰ days_before_today ä¸ªï¼ˆæœ€æ–°çš„ï¼‰
    trading_dates_filtered = calendar[calendar['trade_date'] <= end_dt]['trade_date'].sort_values(ascending=False).head(days_before_today)

    if len(trading_dates_filtered) < days_before_today:
        print(f"âš ï¸ è­¦å‘Š: ä»…æ‰¾åˆ° {len(trading_dates_filtered)} ä¸ªäº¤æ˜“æ—¥ï¼Œå¯ç”¨äº¤æ˜“æ—¥ä¸è¶³ {days_before_today} å¤©")

    start_dt_intraday = trading_dates_filtered.iloc[-1]  # æœ€æ—©çš„æ—¥æœŸåœ¨æœ€åé¢ï¼Œå› ä¸ºæ˜¯é™åº
    start_date = start_dt_intraday.strftime('%Y%m%d')

    return start_date, end_date

def analyze_stocks(config_file: str = 'anylizeconfig.json', keys_file: str = 'keys.json', command_line_stocks: List[str] = None, mode: int = 1):
    """åˆ†æè‚¡ç¥¨çš„ä¸»å‡½æ•°

    :param config_file: é…ç½®æ–‡ä»¶è·¯å¾„
    :param keys_file: å¯†é’¥æ–‡ä»¶è·¯å¾„
    :param command_line_stocks: å‘½ä»¤è¡Œä¼ å…¥çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œå¦‚æœæä¾›åˆ™ä¼˜å…ˆä½¿ç”¨ï¼Œå¦åˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶
    :param mode: int, å¤„ç†æ¨¡å¼ï¼š0=ä»…è·å–æ•°æ®ä¸è¿›è¡Œå¤§æ¨¡å‹åˆ†æï¼Œ1=å®Œæ•´å¤„ç†æµç¨‹ï¼Œå…¶ä»–å€¼=ä¸­æ­¢æ‰§è¡Œ
    """
    # æ£€æŸ¥å¤„ç†æ¨¡å¼
    if mode == 0:
        print("ğŸ“Š æ¨¡å¼0ï¼šä»…è·å–æ•°æ®ï¼Œä¸è¿›è¡Œå¤§æ¨¡å‹åˆ†æ")
    elif mode == 1:
        print("ğŸ¤– æ¨¡å¼1ï¼šå®Œæ•´å¤„ç†æµç¨‹ï¼ˆåŒ…å«å¤§æ¨¡å‹åˆ†æï¼‰")
    else:
        print(f"âŒ æ— æ•ˆçš„å¤„ç†æ¨¡å¼: {mode}ï¼Œä»…æ”¯æŒ0ï¼ˆä»…è·å–æ•°æ®ï¼‰æˆ–1ï¼ˆå®Œæ•´æµç¨‹ï¼‰")
        return

    # 1. è¯»å–é…ç½®
    config = load_config(config_file, keys_file)

    # å¦‚æœæä¾›äº†å‘½ä»¤è¡Œè‚¡ç¥¨å‚æ•°ï¼Œä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼›å¦åˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶
    if command_line_stocks:
        stocks = command_line_stocks
        print(f"ğŸ’´ ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„è‚¡ç¥¨: {', '.join(stocks)}")
    else:
        stocks = select_stocks(config)
        print(f"ğŸ’´ ä½¿ç”¨é…ç½®æ–‡ä»¶æŒ‡å®šçš„è‚¡ç¥¨: {', '.join(stocks)}")

    # è¯»å–æŒ‡å®šçš„ç»“æŸæ—¥æœŸï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨Noneï¼ˆè¡¨ç¤ºä»Šå¤©ï¼‰
    specified_date = config.get('specified_date', '').strip()
    if specified_date:
        print(f"ğŸ“… ä½¿ç”¨æŒ‡å®šçš„ç»“æŸæ—¥æœŸ: {specified_date}")
    else:
        print("ğŸ“… ä½¿ç”¨ä»Šå¤©çš„æ—¥æœŸä½œä¸ºç»“æŸæ—¥æœŸ")
        specified_date = None

    # åˆ†æ—¶æ•°æ®ä½¿ç”¨intraday_daysè®¡ç®—æ—¥æœŸèŒƒå›´ï¼ˆåŸºäºäº¤æ˜“æ—¥ï¼‰
    intraday_days = config['intraday_days']
    intraday_start_date, intraday_end_date = get_intraday_date_range(intraday_days, specified_date)
    print(f"ğŸ“… åˆ†æ—¶æ•°æ®æ—¥æœŸèŒƒå›´: {intraday_start_date} åˆ° {intraday_end_date} (å…±{intraday_days}ä¸ªäº¤æ˜“æ—¥)")

    # å°æ—¶é‡èƒ½æ•°æ®ä½¿ç”¨hourly_volume_daysè®¡ç®—æ—¥æœŸèŒƒå›´ï¼ˆåŸºäºäº¤æ˜“æ—¥ï¼‰
    hourly_volume_days = config.get('hourly_volume_days', intraday_days)  # é»˜è®¤ä½¿ç”¨intraday_days
    hourly_start_date, hourly_end_date = get_intraday_date_range(hourly_volume_days, specified_date)
    print(f"ğŸ“… å°æ—¶é‡èƒ½æ•°æ®æ—¥æœŸèŒƒå›´: {hourly_start_date} åˆ° {hourly_end_date} (å…±{hourly_volume_days}ä¸ªäº¤æ˜“æ—¥)")

    # Kçº¿æ•°æ®ä½¿ç”¨kline_daysè®¡ç®—æ—¥æœŸèŒƒå›´
    kline_days = config.get('kline_days', 60)  # é»˜è®¤60å¤©
    kline_start_date, kline_end_date = get_kline_date_range(kline_days, specified_date)
    print(f"ğŸ“… Kçº¿æ•°æ®æ—¥æœŸèŒƒå›´: {kline_start_date} åˆ° {kline_end_date} (å…±{kline_days}ä¸ªäº¤æ˜“æ—¥)")

    # æ™ºèƒ½é€‰æ‹©prompté…ç½®
    prompt_template = select_prompt_by_model(config)
    print(f"ğŸ¯ ä½¿ç”¨æ–‡ä»¶å¤„ç†æ¨¡å‹ä¸“ç”¨prompt (qwen-long)")

    api_key = config['api_key']  # ä» keys.json è¯»å– API å¯†é’¥
    email_sender = config['email_sender']  # ä» keys.json è¯»å–å‘ä»¶äººé‚®ç®±åœ°å€
    email_password = config['email_password']  # ä» keys.json è¯»å–å‘ä»¶äººé‚®ç®±å¯†ç 
    email_receivers = config['email_receivers']  # ä» keys.json è¯»å–æ”¶ä»¶äººé‚®ç®±åœ°å€

    # 2. å¾ªç¯å¤„ç†æ¯åªè‚¡ç¥¨
    total = len(stocks)
    for index, stock in enumerate(stocks):
        print(f"æ­£åœ¨å¤„ç†è‚¡ç¥¨: {stock} ({index+1}/{total})")
        file_path = None  # åˆå§‹åŒ–æ–‡ä»¶è·¯å¾„
        try:
            # è·å–æ•°æ®å¹¶ä¿å­˜åˆ°CSVæ–‡ä»¶
            result = get_and_save_stock_data(
                stock=stock, 
                start_date=intraday_start_date, 
                end_date=intraday_end_date, 
                kline_days=kline_days,
                hourly_start_date=hourly_start_date,
                hourly_end_date=hourly_end_date
            )
            if result[0] is None:
                print(f"è‚¡ç¥¨ {stock} è·å–æ•°æ®å¤±è´¥ï¼Œè·³è¿‡")
                continue
            file_paths, stock_name = result

            # å¦‚æœæ¨¡å¼ä¸º0ï¼Œä»…è·å–æ•°æ®ï¼Œè·³è¿‡æ–‡ä»¶ä¸Šä¼ å’Œå¤§æ¨¡å‹å¯¹è¯
            if mode == 0:
                print(f"âœ… è‚¡ç¥¨ {stock} æ•°æ®è·å–å®Œæˆï¼Œè·³è¿‡æ–‡ä»¶ä¸Šä¼ å’Œå¤§æ¨¡å‹åˆ†æ")
                continue

            # ä½¿ç”¨åˆå¹¶çš„å®Œæ•´æ–‡ä»¶è¿›è¡Œä¸Šä¼ 
            main_file_path = file_paths['complete']
            file_id = upload_file(file_path=main_file_path, api_key=api_key)
            if file_id is None:
                print(f"è‚¡ç¥¨ {stock} çš„æ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼Œè·³è¿‡")
                continue

            # ä¸é€šä¹‰åƒé—®æ¨¡å‹äº¤äº’ï¼Œç›´æ¥ä¼ é€’å­—å…¸ç±»å‹çš„ prompt_template å’Œé…ç½®å‚æ•°
            response = chat_with_qwen(
                file_id=file_id,
                question=prompt_template,
                api_key=api_key,
                intraday_days=config['intraday_days'],
                kline_days=config['kline_days'],
                stock_code=stock,
                specified_date=specified_date,
                hourly_volume_days=config.get('hourly_volume_days', config['intraday_days'])
            )
            if response:
                print(f"è‚¡ç¥¨ {stock} çš„åˆ†æç»“æœ: {response}\n")

                # ä¿å­˜åˆ†æç»“æœåˆ°MDæ–‡ä»¶
                current_time = datetime.now()
                date_str = current_time.strftime('%Y%m%d')
                time_str = current_time.strftime('%H%M%S')

                # ç¡®ä¿data_outputæ–‡ä»¶å¤¹å­˜åœ¨
                output_dir = get_stock_output_dir(stock)
                output_dir.mkdir(parents=True, exist_ok=True)

                # æ¸…ç†è‚¡ç¥¨åç§°ä¸­çš„ç‰¹æ®Šå­—ç¬¦
                clean_stock_name = stock_name.replace('(', '').replace(')', '').replace(' ', '_')

                md_filename = f"{stock}_{clean_stock_name}_{intraday_start_date}_to_{intraday_end_date}_{date_str}_{time_str}.md"
                md_filepath = output_dir / md_filename

                with open(md_filepath, 'w', encoding='utf-8') as f:
                    f.write(f"# {stock_name}ï¼ˆ{stock}ï¼‰è‚¡ç¥¨åˆ†ææŠ¥å‘Š\n\n")
                    f.write(f"**åˆ†ææ—¶é—´**: {current_time.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}\n\n")
                    f.write(f"---\n\n")
                    f.write(response)

                print(f"âœ… åˆ†æç»“æœå·²ä¿å­˜åˆ°: {md_filepath}")

                # å°†MDæ–‡ä»¶è½¬æ¢ä¸ºHTML
                html_filename = md_filename.replace('.md', '.html')
                html_filepath = output_dir / html_filename
                converter = MarkdownToHTMLConverter()
                if converter.convert_file(str(md_filepath), str(html_filepath)):
                    print(f"âœ… HTMLæ–‡ä»¶å·²ç”Ÿæˆ: {html_filepath}\n")
                else:
                    print(f"âŒ HTMLè½¬æ¢å¤±è´¥: {md_filepath}\n")
                    continue
            else:
                print(f"è‚¡ç¥¨ {stock} çš„èŠå¤©è¯·æ±‚å¤±è´¥ï¼\n")

            # å‘é€é‚®ä»¶å¹¶æ ¹æ®ç»“æœå†³å®šæ˜¯å¦åˆ é™¤æ–‡ä»¶
            print(f"è‚¡ç¥¨ {stock} å‡†å¤‡å‘é€é‚®ä»¶ \n")

            # æå–æŠ•èµ„è¯„çº§å¹¶æ·»åŠ åˆ°é‚®ä»¶ä¸»é¢˜ä¸­
            investment_rating = extract_investment_rating(str(md_filepath))
            if investment_rating:
                email_subject = f"è‚¡ç¥¨ {stock_name}ï¼ˆ{stock}ï¼‰åˆ†æç»“æœ - {investment_rating}"
                print(f"ğŸ“§ é‚®ä»¶ä¸»é¢˜åŒ…å«æŠ•èµ„è¯„çº§: {email_subject}")
            else:
                email_subject = f"è‚¡ç¥¨ {stock_name}ï¼ˆ{stock}ï¼‰åˆ†æç»“æœ"
                print("ğŸ“§ æœªæ‰¾åˆ°æŠ•èµ„è¯„çº§ï¼Œä½¿ç”¨é»˜è®¤é‚®ä»¶ä¸»é¢˜")

            # å‡†å¤‡é‚®ä»¶æ­£æ–‡
            email_body = f"è‚¡ç¥¨ {stock_name}ï¼ˆ{stock}ï¼‰çš„åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆï¼Œè¯·æŸ¥çœ‹é™„ä»¶ä¸­çš„æ–‡ä»¶ã€‚\n\né™„ä»¶åŒ…å«ï¼š\n1. ä¸»åˆ†ææŠ¥å‘Šï¼ˆHTMLæ ¼å¼ï¼‰\n2. å°æ—¶é‡èƒ½åˆ†ææ•°æ®å·²åŒ…å«åœ¨CSVæ–‡ä»¶ä¸­"

            # å‡†å¤‡é™„ä»¶åˆ—è¡¨ - åªåŒ…å«HTMLæ–‡ä»¶
            attachment_list = [str(html_filepath)]  # åªå‘é€HTMLæ–‡ä»¶

            send_email(
                subject=email_subject,
                body=email_body,
                receivers=email_receivers,
                sender=email_sender,
                password=email_password,
                attachment_paths=attachment_list  # åªå‘é€HTMLé™„ä»¶
            )

        except Exception as e:
            print(f"å¤„ç†è‚¡ç¥¨ {stock} æ—¶å‡ºé”™: {e}\n")

        if index < total - 1:
            for i in range(10):  # ç­‰å¾… 300 ç§’ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                print(".", end="", flush=True)
                t.sleep(1)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹

# è¿è¡Œç¨‹åº
if __name__ == "__main__":
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        # sys.argv[0] æ˜¯è„šæœ¬å
        # sys.argv[1] æ˜¯å¤„ç†æ¨¡å¼ï¼ˆ0æˆ–1ï¼‰
        # sys.argv[2:] æ˜¯è‚¡ç¥¨ä»£ç åˆ—è¡¨
        
        try:
            mode = int(sys.argv[1])
            if mode not in [0, 1]:
                print(f"âŒ æ— æ•ˆçš„å¤„ç†æ¨¡å¼: {mode}")
                print("ğŸ’¡ ä½¿ç”¨æ–¹æ³•: python anaByQwen2.py <mode> [è‚¡ç¥¨ä»£ç 1] [è‚¡ç¥¨ä»£ç 2] ...")
                print("   mode: 0=ä»…è·å–æ•°æ®ï¼Œ1=å®Œæ•´æµç¨‹")
                print("   ç¤ºä¾‹: python anaByQwen2.py 0 600000 000001")
                sys.exit(1)
            
            command_line_stocks = sys.argv[2:] if len(sys.argv) > 2 else None
            
            if command_line_stocks:
                print(f"ğŸ”§ æ£€æµ‹åˆ°å‘½ä»¤è¡Œå‚æ•°ï¼Œæ¨¡å¼: {mode}, è‚¡ç¥¨ä»£ç : {', '.join(command_line_stocks)}")
            else:
                print(f"ğŸ”§ æ£€æµ‹åˆ°å‘½ä»¤è¡Œå‚æ•°ï¼Œæ¨¡å¼: {mode}, ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è‚¡ç¥¨è®¾ç½®")
            
            analyze_stocks('anylizeconfig.json', 'keys.json', command_line_stocks, mode)
            
        except ValueError:
            print(f"âŒ å¤„ç†æ¨¡å¼å¿…é¡»æ˜¯æ•°å­—: {sys.argv[1]}")
            print("ğŸ’¡ ä½¿ç”¨æ–¹æ³•: python anaByQwen2.py <mode> [è‚¡ç¥¨ä»£ç 1] [è‚¡ç¥¨ä»£ç 2] ...")
            print("   mode: 0=ä»…è·å–æ•°æ®ï¼Œ1=å®Œæ•´æµç¨‹")
            print("   ç¤ºä¾‹: python anaByQwen2.py 0 600000 000001")
            sys.exit(1)
    else:
        print("ğŸ”§ æœªæ£€æµ‹åˆ°å‘½ä»¤è¡Œå‚æ•°ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å¼1å’Œé…ç½®æ–‡ä»¶ä¸­çš„è‚¡ç¥¨è®¾ç½®")
        analyze_stocks('anylizeconfig.json', 'keys.json', None, 1)
